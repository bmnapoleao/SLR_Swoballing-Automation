@article{1994,
 abstract = {},
 doi = {10.1016/0024-6301(94)90262-3},
 journal = {Long Range Planning},
 month = {oct},
 number = {5},
 pages = {163},
 publisher = {Elsevier {BV}},
 title = {Knowledge and value: A new perspective on corporate transformation},
 url = {https://doi.org/10.1016%2F0024-6301%2894%2990262-3},
 volume = {27},
 year = {1994}
}

@article{Brownsword_2000,
 abstract = {As the use of commercial technology and products in systems becomes increasingly popular, application developers must develop a better understanding of the dynamic principles and processes for system creation and maintenance. Currently, they have little information on how COTS products affect existing system development processes or what new processes are needed. These authors from the Software Engineering Institute articulate the new and changed process elements for developing and maintaining COTS-based systems.},
 author = {L. Brownsword and T. Oberndorf and C.A. Sledge},
 doi = {10.1109/52.854068},
 journal = {{IEEE} Software},
 number = {4},
 pages = {48--55},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Developing new processes for {COTS}-based systems},
 url = {https://doi.org/10.1109%2F52.854068},
 volume = {17},
 year = {2000}
}

@article{Ebner_2002,
 abstract = {In reengineering, the authors argue for complete traceability from code through specifications to code. In the course of reverse
engineering a legacy software system and its subsequent redesign and
reimplementation, they found several cases where traceability provided
immediate benefits that appear to be specific to reengineering},
 author = {G. Ebner and H. Kaindl},
 doi = {10.1109/ms.2002.1003459},
 journal = {{IEEE} Software},
 month = {may},
 number = {3},
 pages = {70--77},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Tracing all around in reengineering},
 url = {https://doi.org/10.1109%2Fms.2002.1003459},
 volume = {19},
 year = {2002}
}

@inproceedings{El_Emam,
 abstract = {To make recommendations for improving requirements engineering processes, it is critical to understand the problems faced in contemporary practice. We describe a field study whose general objectives were to formulate recommendations to practitioners for improving requirements engineering processes, and to provide directions for future research on methods and tools. The results indicate that there are seven key issues of greatest concern in requirements engineering practice. These issues are discussed in terms of the problems they represent, how these problems are addressed successfully in practice, and impediments to the implementation of such good practices.},
 author = {K. El Emam and N.H. Madhavji},
 booktitle = {Proceedings of 1995 {IEEE} International Symposium on Requirements Engineering ({RE}{\textquotesingle}95)},
 doi = {10.1109/isre.1995.512547},
 publisher = {{IEEE} Comput. Soc. Press},
 title = {A field study of requirements engineering practices in information systems development},
 url = {https://doi.org/10.1109%2Fisre.1995.512547}
}

@inproceedings{Farbey,
 abstract = {The paper argues that there are new insights to be gained from a strategic analysis of requirements engineering. The paper is motivated by a simple question: what does it take to be a world class software acquirer? The question has relevance for requirements engineers because for many organisations market pressures mean that software is commonly acquired rather than developed from scratch. The paper builds on the work of C. H. Fine (1998) who suggests that product, process and supply chain should be designed together, i.e., 3D concurrent engineering. Using a number of reference theories, it proposes a systematic way of carrying out 3D concurrent engineering. The paper concludes that the critical activity in supply chain design is the design of the distribution of skills and the nature of contracts},
 author = {B. Farbey and A. Finkelstein},
 booktitle = {Proceedings Fifth {IEEE} International Symposium on Requirements Engineering},
 doi = {10.1109/isre.2001.948546},
 publisher = {{IEEE} Comput. Soc},
 title = {Software acquisition: a business strategy analysis},
 url = {https://doi.org/10.1109%2Fisre.2001.948546}
}

@article{Fichman_1997,
 abstract = {The burden of organizational learning surrounding software process innovations (SPIs)---and complex organizational technologies in general---creates a "knowledge barrier" that inhibits diffusion. Attewell (Attewell, P. 1992. Technology diffusion and organizational learning the case of business computing. Organ. Sci. 3(1) 1--19.) has suggested that many organizations will defer adoption until knowledge barriers have been sufficiently lowered; however, this leaves open the question of which organizations should be more likely to innovate, even in face of high knowledge barriers. It is proposed here that organizations will innovate in the presence of knowledge barriers when the burden of organizational learning is effectively lower, either because much of the required know-how already exists within the organization, or because such knowledge can be acquired more easily or more economically. Specifically, it is hypothesized that organizations will have a greater propensity to initiate and sustain the assimilation of SPIs when they have a greater scale of activities over which learning costs can be spread (learning-related scale), more extensive existing knowledge related to the focal innovation (related knowledge), and a greater diversity of technical knowledge and activities (diversity). An empirical study using data on the assimilation of object-oriented programming languages (OOPLs) by 608 information technology organizations strongly confirmed the importance of the three hypothesized factors in explaining the assimilation of OOPLs.},
 author = {Robert G. Fichman and Chris F. Kemerer},
 doi = {10.1287/mnsc.43.10.1345},
 journal = {Management Science},
 month = {oct},
 number = {10},
 pages = {1345--1363},
 publisher = {Institute for Operations Research and the Management Sciences ({INFORMS})},
 title = {The Assimilation of Software Process Innovations: An Organizational Learning Perspective},
 url = {https://doi.org/10.1287%2Fmnsc.43.10.1345},
 volume = {43},
 year = {1997}
}

@inproceedings{Fowler,
 abstract = {We built a prototype “transition package” to determine
if adoption of requirements management practices would be expedited by a
suite of RM-specific materials for change agents. The transition package
was a password-protected web site that included about 100 documents for
use in both performing and introducing requirements management
practices. These documents included examples, templates, checklists, and
guidance materials. Three frameworks-document type, Software Capability
Maturity Model Common Feature, and technology transition process
model-helped users make use of the documents. This paper describes the
prototype, summarizes the data gathered from an evaluation by reviewers
and piloting organizations, and comments on what the data reflect about
the need for and use of technology to support requirements management.
Finally, the potential of this approach for improving software
technology adoption and process improvement within organizations is
assessed based on these experiences},
 author = {P. Fowler and M. Patrick and A. Carleton and B. Merrin},
 booktitle = {Proceedings of {IEEE} International Symposium on Requirements Engineering: {RE} {\textquotesingle}98},
 doi = {10.1109/icre.1998.667819},
 publisher = {{IEEE} Comput. Soc},
 title = {Transition packages: an experiment in expediting the introduction of requirements management},
 url = {https://doi.org/10.1109%2Ficre.1998.667819}
}

@incollection{Fowler_1997,
 abstract = {This paper describes the concept of ‘transition packages,’ a proposed approach to practical technology introduction for software organizations, and why we chose to evaluate this concept in the context of the Software Capability Maturity Model. The paper also describes how this concept was considered in a practitioner-oriented workshop based on benchmarking principles, the processes and findings of that workshop, and the outputs of participant exercises. The final section provides analysis of the fmdings and suggests possible next steps in the creation of the first Requirements Management transition package for software engineering community use.},
 author = {Priscilla Fowler and Mac Patrick},
 booktitle = {{IFIP} Advances in Information and Communication Technology},
 doi = {10.1007/978-0-387-35092-9_19},
 pages = {304--318},
 publisher = {Springer {US}},
 title = {Experience Using a Benchmarking Workshop to Explore One Route to Practical Technology Introduction},
 url = {https://doi.org/10.1007%2F978-0-387-35092-9_19},
 year = {1997}
}

@article{Heitmeyer_1996,
 abstract = {This article describes a formal analysis technique, called consistency checking, for automatic detection of errors, such as type errors, nondeterminism, missing cases, and circular definitions, in requirements specifications. The technique is designed to analyze requirements specifications expressed in the SCR (Software Cost Reduction) tabular notation. As background, the SCR approach to specifying requirements is reviewed. To provide a formal semantics for the SCR notation and a foundation for consistency checking, a formal requirements model is introduced; the model represents a software system as a finite-state automaton, which produces externally visible outputs in response to changes in monitored environmental quantities. Results of two experiments are presented which evaluated the utility and scalability of our technique for consistency checking in a real-world avionics application. The role of consistency checking during the requirements phase of software development is discussed.},
 author = {Constance L. Heitmeyer and Ralph D. Jeffords and Bruce G. Labaw},
 doi = {10.1145/234426.234431},
 journal = {{ACM} Transactions on Software Engineering and Methodology},
 month = {jul},
 number = {3},
 pages = {231--261},
 publisher = {Association for Computing Machinery ({ACM})},
 title = {Automated consistency checking of requirements specifications},
 url = {https://doi.org/10.1145%2F234426.234431},
 volume = {5},
 year = {1996}
}

@article{Heitmeyer_1998,
 abstract = {Exposing inconsistencies can uncover many defects in software
specifications. One approach to exposing inconsistencies analyzes two
redundant specifications, one operational and the other property-based,
and reports discrepancies. This paper describes a
“practical” formal method, based on this approach and the
SCR (software cost reduction) tabular notation, that can expose
inconsistencies in software requirements specifications. Because users
of the method do not need advanced mathematical training or
theorem-proving skills, most software developers should be able to apply
the method without extraordinary effort. This paper also describes an
application of the method which exposed a safety violation in the
contractor-produced software requirements specification of a sizable,
safety-critical control system. Because the enormous state space of
specifications of practical software usually renders direct analysis
impractical, a common approach is to apply abstraction to the
specification. To reduce the state space of the control system
specification, two “pushbutton” abstraction methods were
applied, one which automatically removes irrelevant variables and a
second which replaces the large, possibly infinite, type sets of certain
variables with smaller type sets. Analyzing the reduced specification
with the model checker Spin uncovered a possible safety violation.
Simulation demonstrated that the safety violation was not spurious but
an actual defect in the original specification},
 author = {C. Heitmeyer and J. Kirby and B. Labaw and M. Archer and R. Bharadwaj},
 doi = {10.1109/32.730543},
 journal = {{IEEE} Transactions on Software Engineering},
 month = {nov},
 number = {11},
 pages = {927--948},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Using abstraction and model checking to detect safety violations in requirements specifications},
 url = {https://doi.org/10.1109%2F32.730543},
 volume = {24},
 year = {1998}
}

@article{Hsia_1993,
 abstract = {It is argued that, in general, requirements engineering produces one large document, written in a natural language, that few people bother to read. Projects that do read and follow the document often build systems that do not satisfy needs. The reasons for the current state of the practice are listed. Research areas that have significant payoff potential, including improving natural-language specifications, rapid prototyping and requirements animation, requirements clustering, requirements-based testing, computer-aided requirements engineering, requirements reuse, research into methods, knowledge engineering, formal methods, and a unified framework, are outlined.< >},
 author = {P. Hsia and A.M. Davis and D.C. Kung},
 doi = {10.1109/52.241974},
 journal = {{IEEE} Software},
 month = {nov},
 number = {6},
 pages = {75--79},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Status report: requirements engineering},
 url = {https://doi.org/10.1109%2F52.241974},
 volume = {10},
 year = {1993}
}

@article{Jarke_1998,
 abstract = {Scenario management (SM) means different things to different people, even though everyone seems to admit its current importance and its further potential. In this paper, we seek to provide an interdisciplinary framework for SM from three major disciplines that use scenarios - strategic management, human-computer interaction, and software and systems engineering - to deal with description of current and future realities. In particular, we attempt to answer to the following questions: How are scenarios developed and used in each of the three disciplines? Why are they becoming important? What are current research contributions in scenario management? What are the research and practical issues related to the creation and use of scenarios, in particular in the area of requirements engineering? Based on brainstorming techniques, this paper proposes an interdisciplinary definition of scenarios, frameworks for scenario development, use and evaluation, and directions for future research.},
 author = {Matthias Jarke and X. Tung Bui and John M. Carroll},
 doi = {10.1007/s007660050002},
 journal = {Requirements Engineering},
 month = {mar},
 number = {3-4},
 pages = {155--173},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Scenario Management: An Interdisciplinary Approach},
 url = {https://doi.org/10.1007%2Fs007660050002},
 volume = {3},
 year = {1998}
}

@inproceedings{Kaindl,
 abstract = {The goal of this panel is to really understand the issue of why it is so difficult to introduce research results from requirements engineering into mainstream practice. The result should be a (research) agenda that helps us to bridge the gap between theory and practice and, finally, to reach the people in the trenches so that they improve the way they deal with requirements.},
 author = {H. Kaindl},
 booktitle = {Proceedings Fourth International Conference on Requirements Engineering. {ICRE} 2000. (Cat. No.98TB100219)},
 doi = {10.1109/icre.2000.855588},
 publisher = {{IEEE} Comput. Soc},
 title = {Why is it so difficult to introduce requirements engineering research results into mainstream requirements engineering practice?},
 url = {https://doi.org/10.1109%2Ficre.2000.855588}
}

@article{Kaindl_1997,
 abstract = {According to our experience in real‐world projects, we still observe deficiencies of current methods for object‐oriented analysis (OOA), especially in respect to the early elicitation and definition of requirements. Therefore, we used object‐oriented technology and hypertext to develop a practical approach – with tool support – that tightly combines OOA with requirements definition. This novel approach is compatible with virtually any OOA method. While more work needs to be done especially for supporting the process of requirements definition, the observed deficiencies and current limitations of existing OOA methods are addressed and partly removed through this combination. We have applied our approach in real‐world projects, and our experience suggests the usefulness of this approach. Essentially, its use leads to a more complete and structured definition of the requirements, and consequently we derive some recommendations for practitioners.},
 author = {Hermann Kaindl},
 doi = {10.1023/a:1018954425162},
 journal = {Annals of Software Engineering},
 pages = {319--343},
 publisher = {Springer Science and Business Media {LLC}},
 url = {https://doi.org/10.1023%2Fa%3A1018954425162},
 volume = {3},
 year = {1997}
}

@article{Kaindl_1999,
 abstract = {The transition from object-oriented (OO) analysis to OO design is
indeed difficult. Unfortunately, most of the literature on this topic
has incorrectly assessed this issue since the notion of OO analysis was
introduced about a decade ago},
 author = {H. Kaindl},
 doi = {10.1109/52.795107},
 journal = {{IEEE} Software},
 number = {5},
 pages = {94--102},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Difficulties in the transition from {OO} analysis to design},
 url = {https://doi.org/10.1109%2F52.795107},
 volume = {16},
 year = {1999}
}

@article{Kaindl_2000,
 abstract = {While promising approaches to early system design using scenarios
have been proposed, no design process is available that guides
scenario-based design. We present a model that combines scenarios both
with functions and goals. Functions are required to make the desired
behavior of some scenario happen in order to achieve one or more goals.
Using this model, we propose a systematic and concrete design process
that is both model-driven and data-driven. Our design process supports
the transition from the current to a new system and guides the design of
a new system. In addition, this process makes it possible to detect a
certain kind of redundancy and to improve both completeness and
understandability of the resulting design. We have applied our approach
in real-world projects, and our experience suggests the utility of this
approach},
 author = {H. Kaindl},
 doi = {10.1109/3468.867861},
 journal = {{IEEE} Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
 number = {5},
 pages = {537--551},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {A design process based on a model combining scenarios with goals and functions},
 url = {https://doi.org/10.1109%2F3468.867861},
 volume = {30},
 year = {2000}
}

@inproceedings{Kirby,
 abstract = {To date, the tabular-based SCR (Software Cost Reduction) method has been applied mostly to the development of embedded control systems. This paper describes the successful application of the SCR method, including the SCR* toolset, to a different class of system, a COMSEC (Communications Security) device called CD that must correctly manage encrypted communications. The paper summarizes how the tools in SCR* were used to validate and to debug the SCR specification and to demonstrate that the specification satisfies a set of critical security properties. The development of the CD specification involved many tools in SCR*: a specification editor, a consistency checker, a simulator, the TAME interface to the theorem prover PVS, and various other analysis tools. Our experience provides evidence that use of the SCR* toolset to develop high-quality requirements specifications of moderately complex COMSEC systems is both practical and low-cost. 1 Introduction COMSEC (Communications Security)...},
 author = {J. Kirby and M. Archer and C. Heitmeyer},
 booktitle = {Proceedings 15th Annual Computer Security Applications Conference ({ACSAC}{\textquotesingle}99)},
 doi = {10.1109/csac.1999.816018},
 publisher = {{IEEE} Comput. Soc},
 title = {{SCR}: a practical approach to building a high assurance {COMSEC} system},
 url = {https://doi.org/10.1109%2Fcsac.1999.816018}
}

@article{Leite_1997,
 abstract = {Scenarios are well recognised as an important strategy towards understanding the interface between the environment and the
system as well as a means of eliciting and specifying software behaviour. We adopt a broader view of scenarios. For us, a
scenario is an evolving description of situations in the environment. Our proposal is framed by Leite’s work on a client-oriented
requirements baseline, which aims to model the external requirements of a software system and its evolution. Scenarios start
by describing the environment situations, according to the main actions performed outside the software system. Scenarios also
help to clarify the interrelation between functional and non-functional requirements. We have validated our strategy and the
related representations based on case studies.},
 author = {Julio Cesar Sampaio do Prado Leite and Gustavo Rossi and Federico Balaguer and Vanesa Maiorana and Gladys Kaplan and Graciela Hadad and Alejandro Oliveros},
 doi = {10.1007/bf02745371},
 journal = {Requirements Engineering},
 month = {dec},
 number = {4},
 pages = {184--198},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Enhancing a requirements baseline with scenarios},
 url = {https://doi.org/10.1007%2Fbf02745371},
 volume = {2},
 year = {1997}
}

@article{Leveson_1994,
 abstract = {The paper describes an approach to writing requirements specifications for process-control systems, a specification language that supports this approach, and an example application of the approach and the language on an industrial aircraft collision avoidance system (TCAS II). The example specification demonstrates: the practicality of writing a formal requirements specification for a complex, process-control system; and the feasibility of building a formal model of a system using a specification language that is readable and reviewable by application experts who are not computer scientists or mathematicians. Some lessons learned in the process of this work, which are applicable both to forward and reverse engineering, are also presented},
 author = {N.G. Leveson and M.P.E. Heimdahl and H. Hildreth and J.D. Reese},
 doi = {10.1109/32.317428},
 journal = {{IEEE} Transactions on Software Engineering},
 number = {9},
 pages = {684--707},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Requirements specification for process-control systems},
 url = {https://doi.org/10.1109%2F32.317428},
 volume = {20},
 year = {1994}
}

@incollection{MADDERS_1983,
 abstract = {This chapter describes the establishment and role of European Space Agency (ESA). The ESA is the successor to the European Launcher Development Organization and the European Space Research Organization, which were created under separate conventions concluded in 1962 as European bases for the execution of space activities and programs. ESA was created by the Convention for the Establishment of a European Space Agency, which was opened for signature in Paris on May 30, 1975. The Director General, who is appointed by the council, acts as ESA's chief executive officer and legal representative. He reports annually to the council, attends its meetings, and may make proposals but has no vote. The administrative structure beneath the Director General consists of function oriented Directorates for Administration, Applications Programmes, Spacecraft Operations, Scientific Programmes, and Space Transportation Systems, as well as the Technical Directorate.},
 author = {KEVIN J. MADDERS},
 booktitle = {Regional Cooperation, Organizations and Problems},
 doi = {10.1016/b978-0-444-86237-2.50064-0},
 pages = {203--206},
 publisher = {Elsevier},
 title = {{EUROPEAN} {SPACE} {AGENCY}},
 url = {https://doi.org/10.1016%2Fb978-0-444-86237-2.50064-0},
 year = {1983}
}

@inproceedings{Mead,
 abstract = {Not Available},
 author = {N.R. Mead},
 booktitle = {Proceedings Fourth International Conference on Requirements Engineering. {ICRE} 2000. (Cat. No.98TB100219)},
 doi = {10.1109/icre.2000.855592},
 publisher = {{IEEE} Comput. Soc},
 title = {Why is it so difficult to introduce requirements engineering research results into mainstream requirements engineering practice?},
 url = {https://doi.org/10.1109%2Ficre.2000.855592}
}

@article{Morris_1998,
 abstract = {In September 1996 the Joint Research Centre of the European Commission held a workshop in Brussels on problems with industrial uptake from research and development projects in Requirements Engineering. Although there have, in this domain, been a number of research projects industrial uptake has rarely lived up to expectations. The workshop set out to investigate possible explanations for this and what potential mechanisms there may be for promoting industrial uptake of current and future Requirement Engineering projects. This paper has two functions: to describe the results of this workshops and to provide a framework to support planning for future RE activities and initiatives.},
 author = {Philip Morris and Marcelo Masera and Marc Wilikens},
 doi = {10.1007/bf02919966},
 journal = {Requirements Engineering},
 month = {jun},
 number = {2},
 pages = {79--83},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Requirements engineering and industrial uptake},
 url = {https://doi.org/10.1007%2Fbf02919966},
 volume = {3},
 year = {1998}
}

@article{Ramesh_2001,
 abstract = {Requirements traceability is intended to ensure continued alignment between stakeholder requirements and various outputs of the system development process. To be useful, traces must be organized according to some modeling framework. Indeed, several such frameworks have been proposed, mostly based on theoretical considerations or analysis of other literature. This paper, in contrast, follows an empirical approach. Focus groups and interviews conducted in 26 major software development organizations demonstrate a wide range of traceability practices with distinct low-end and high-end users of traceability. From these observations, reference models comprising the most important kinds of traceability links for various development tasks have been synthesized. The resulting models have been validated in case studies and are incorporated in a number of traceability tools. A detailed case study on the use of the models is presented. Four kinds of traceability link types are identified and critical issues that must be resolved for implementing each type and potential solutions are discussed. Implications for the design of next-generation traceability methods and tools are discussed and illustrated},
 author = {B. Ramesh and M. Jarke},
 doi = {10.1109/32.895989},
 journal = {{IEEE} Transactions on Software Engineering},
 number = {1},
 pages = {58--93},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Toward reference models for requirements traceability},
 url = {https://doi.org/10.1109%2F32.895989},
 volume = {27},
 year = {2001}
}

@article{Siddiqi_1996,
 abstract = {Developments in requirements engineering have come in waves. The requirements engineering has benefited from both the information-systems and software-engineering paradigms. Today, a number of approaches judiciously mix techniques borrowed from both strands. This article addresses the factors that need to be addressed for further developments. In particular, it determines the activities that should be included in requirements engineering, the constituents of a requirement, and the issues of practice that need further attention.},
 author = {J. Siddiqi},
 doi = {10.1109/ms.1996.506458},
 journal = {{IEEE} Software},
 month = {mar},
 number = {2},
 pages = {15},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Requirement engineering: The emerging wisdom [Guest Editor{\textquotesingle}s introduction]},
 url = {https://doi.org/10.1109%2Fms.1996.506458},
 volume = {13},
 year = {1996}
}

@article{Siddiqi_1997,
 abstract = {Assuring a high quality requirements specification document involves both an early validation process and an increased level
of participation. An approach and its supporting environment which combines the benefits of a formal system specification
and its subsequent execution via a rapid prototype is reported. The environment assists in the construction, clarification,
validation and visualisation of a formal specification. An illustrative case study demonstrates the consequences of assertions
about system properties at this early stage of software development. Our approach involves the pragmatic combination of technical
benefits of formal systems engineering based techniques with the context‐sensitive notions of increased participation of both
developer and user stakeholders to move us closer towards a quality requirements specification document.},
 author = {Jawed I. Siddiqi and Ian C. Morrey and Chris R. Roast and Mehmet B. Ozcan},
 doi = {10.1023/a:1018977602872},
 journal = {Annals of Software Engineering},
 pages = {131--155},
 publisher = {Springer Science and Business Media {LLC}},
 url = {https://doi.org/10.1023%2Fa%3A1018977602872},
 volume = {3},
 year = {1997}
}

@article{Wilmott_2003,
 abstract = {},
 author = {Paul Wilmott},
 doi = {10.1002/wilm.42820030602},
 journal = {Wilmott},
 month = {nov},
 number = {6},
 pages = {2--3},
 publisher = {Wilmott Magazine Ltd},
 title = {You can{\textquotesingle}t have one without the other},
 url = {https://doi.org/10.1002%2Fwilm.42820030602},
 volume = {2003},
 year = {2003}
}

@inproceedings{Yu,
 abstract = {Requirements are usually understood as stating what a system is
supposed to do, as apposed to how it should do it. However,
understanding the organizational context and rationales (the
“Whys”) that lead up to systems requirements can be just as
important for the ongoing success of the system. Requirements modelling
techniques can be used to help deal with the knowledge and reasoning
needed in this earlier phase of requirements engineering. However most
existing requirements techniques are intended more for the later phase
of requirements engineering, which focuses on completeness, consistency,
and automated verification of requirements. In contrast, the early phase
aims to model and analyze stakeholder interests and how they might be
addressed, or compromised, by various system-and-environment
alternatives. This paper argues, therefore, that a different kind of
modelling and reasoning support is needed for the early phase. An
outline of the i* framework is given as an example of a step in this
direction. Meeting scheduling is used as a domain example},
 author = {E.S.K. Yu},
 booktitle = {Proceedings of {ISRE} {\textquotesingle}97: 3rd {IEEE} International Symposium on Requirements Engineering},
 doi = {10.1109/isre.1997.566873},
 publisher = {{IEEE} Comput. Soc. Press},
 title = {Towards modelling and reasoning support for early-phase requirements engineering},
 url = {https://doi.org/10.1109%2Fisre.1997.566873}
}

@article{_mite_2012,
 abstract = {},
 author = {Darja {\v{S}}mite and Claes Wohlin and Zane Galvi{\c{n}}a and Rafael Prikladnicki},
 doi = {10.1007/s10664-012-9217-9},
 journal = {Empirical Software Engineering},
 month = {jul},
 number = {1},
 pages = {105--153},
 publisher = {Springer Science and Business Media {LLC}},
 title = {An empirically based terminology and taxonomy for global software engineering},
 url = {https://doi.org/10.1007%2Fs10664-012-9217-9},
 volume = {19},
 year = {2012}
}

@inproceedings{Ali_2012,
 abstract = {},
 author = {Nauman bin Ali and Kai Petersen and Mika Mäntylä},
 booktitle = {Proceedings of the {ACM}-{IEEE} international symposium on Empirical software engineering and measurement},
 doi = {10.1145/2372251.2372290},
 month = {sep},
 publisher = {{ACM}},
 title = {Testing highly complex system of systems},
 url = {https://doi.org/10.1145%2F2372251.2372290},
 year = {2012}
}

@article{Baca_2013,
 abstract = {Software security risk analysis is an important part of improving software quality. In previous research we proposed countermeasure graphs (CGs), an approach to conduct risk analysis, combining the ideas of different risk analysis approaches. The approach was designed for reuse and easy evolvability to support agile software development. CGs have not been evaluated in industry practice in agile software development. In this research we evaluate the ability of CGs to support practitioners in identifying the most critical threats and countermeasures. The research method used is participatory action research where CGs were evaluated in a series of risk analyses on four different telecom products. With Peltier (used prior to the use of CGs at the company) the practitioners identified attacks with low to medium risk level. CGs allowed practitioners to identify more serious risks (in the first iteration 1 serious threat, 5 high risk threats, and 11 medium threats). The need for tool support was identified very early, tool support allowed the practitioners to play through scenarios of which countermeasures to implement, and supported reuse. The results indicate that CGs support practitioners in identifying high risk security threats, work well in an agile software development context, and are cost-effective.},
 author = {Dejan Baca and Kai Petersen},
 doi = {10.1016/j.jss.2013.04.023},
 journal = {Journal of Systems and Software},
 month = {sep},
 number = {9},
 pages = {2411--2428},
 publisher = {Elsevier {BV}},
 title = {Countermeasure graphs for software security risk assessment: An action research},
 url = {https://doi.org/10.1016%2Fj.jss.2013.04.023},
 volume = {86},
 year = {2013}
}

@inproceedings{Do_2008,
 abstract = {Regression testing is an expensive process used to validate modi- fied software. Test case prioritization techniques improve the cost- effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing p rocess. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, howe ver, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. Therefore, we conducted an experiment to assess the effects of time constraints on the c osts and benefits of prioritization techniques. Our results show tha t time constraints can indeed play a significant role in determinin g both the cost-effectiveness of prioritization, and the relativ e cost-benefit tradeoffs among techniques, with important implications for the use of prioritization in practice.},
 author = {Hyunsook Do and Siavash Mirarab and Ladan Tahvildari and Gregg Rothermel},
 booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} International Symposium on Foundations of software engineering},
 doi = {10.1145/1453101.1453113},
 month = {nov},
 publisher = {{ACM}},
 title = {An empirical study of the effect of time constraints on the cost-benefits of regression testing},
 url = {https://doi.org/10.1145%2F1453101.1453113},
 year = {2008}
}

@article{Dyba_2013,
 abstract = {What works for whom, where, when, and why is the ultimate question of evidence-based software engineering. Still, the empirical research seems mostly concerned with identifying universal relationships that are independent of how work settings and other contexts interact with the processes important to software practice. Questions of “What is best?” seem to prevail. For example, “Which is better: pair or solo programming? test-first or test-last?” However, just as the question of whether a helicopter is better than a bicycle is meaningless, so are these questions because the answers depend on the settings and goals of the projects studied. Practice settings are rarely, if ever, the same. For example, the environments of software organizations differ, as do their sizes, customer types, countries or geography, and history. All these factors influence engineering practices in unique ways. Additionally, the human factors underlying the organizational culture differ from one organization to the next and also influence the way software is developed. We know these issues and the ways they interrelate are important for the successful uptake of research into practice. However, the nature of these relationships is poorly understood. Consequently, we can't a priori assume that the results of a particular study apply outside the specific context in which it was run. Here, I offer an overview of how context affects empirical research and how to better contextualize empirical evidence so that others can better understand what works for whom, where, when, and why.},
 author = {Tore Dyba},
 doi = {10.1109/ms.2013.4},
 journal = {{IEEE} Software},
 month = {jan},
 number = {1},
 pages = {81--83},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Contextualizing empirical evidence},
 url = {https://doi.org/10.1109%2Fms.2013.4},
 volume = {30},
 year = {2013}
}

@article{Glass_2009,
 abstract = {This article is a background report describing a comprehensive study of research in the three computing disciplines Computer Science, Software Engineering, and Information Systems. Findings relate to research topics, approaches, methods, reference disciplines, and levels of analysis. The article informally describes the process used and the research products produced.},
 author = {Robert L. Glass and Iris Vessey and V. Ramesh},
 doi = {10.1016/j.infsof.2008.09.015},
 journal = {Information and Software Technology},
 month = {jan},
 number = {1},
 pages = {68--70},
 publisher = {Elsevier {BV}},
 title = {{RESRES}: The story behind the paper {\textquotedblleft}Research in software engineering: An analysis of the literature{\textquotedblright}},
 url = {https://doi.org/10.1016%2Fj.infsof.2008.09.015},
 volume = {51},
 year = {2009}
}

@article{Gorschek_2006,
 abstract = {Technology transfer, and thus industry-relevant research, involves more than merely producing research results and delivering them in publications and technical reports. It demands close cooperation and collaboration between industry and academia throughout the entire research process. During research conducted in a partnership between Blekinge Institute of Technology and two companies, Danaher Motion Saro AB (DHR) and ABB, we devised a technology transfer model that embodies this philosophy. We initiated this partnership to conduct industry-relevant research in requirements engineering and product management. Technology transfer in this context is a prerequisite: it validates academic research results in a real setting, and it provides a way to improve industry development and business processes},
 author = {Tony Gorschek and Per Garre and Stig Larsson and Claes Wohlin},
 doi = {10.1109/ms.2006.147},
 journal = {{IEEE} Software},
 month = {nov},
 number = {6},
 pages = {88--95},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {A Model for Technology Transfer in Practice},
 url = {https://doi.org/10.1109%2Fms.2006.147},
 volume = {23},
 year = {2006}
}

@article{Hannay_2009,
 abstract = {Several experiments on the effects of pair versus solo programming have been reported in the literature. We present a meta-analysis of these studies. The analysis shows a small significant positive overall effect of pair programming on quality, a medium significant positive overall effect on duration, and a medium significant negative overall effect on effort. However, between-study variance is significant, and there are signs of publication bias among published studies on pair programming. A more detailed examination of the evidence suggests that pair programming is faster than solo programming when programming task complexity is low and yields code solutions of higher quality when task complexity is high. The higher quality for complex tasks comes at a price of considerably greater effort, while the reduced completion time for the simpler tasks comes at a price of noticeably lower quality. We conclude that greater attention should be given to moderating factors on the effects of pair programming.},
 author = {Jo E. Hannay and Tore Dyb{\aa} and Erik Arisholm and Dag I.K. Sj{\o}berg},
 doi = {10.1016/j.infsof.2009.02.001},
 journal = {Information and Software Technology},
 month = {jul},
 number = {7},
 pages = {1110--1122},
 publisher = {Elsevier {BV}},
 title = {The effectiveness of pair programming: A meta-analysis},
 url = {https://doi.org/10.1016%2Fj.infsof.2009.02.001},
 volume = {51},
 year = {2009}
}

@article{Ivarsson_2010,
 abstract = {One of the main goals of an applied research field such as software engineering is the transfer and widespread use of research
results in industry. To impact industry, researchers developing technologies in academia need to provide tangible evidence
of the advantages of using them. This can be done trough step-wise validation, enabling researchers to gradually test and
evaluate technologies to finally try them in real settings with real users and applications. The evidence obtained, together
with detailed information on how the validation was conducted, offers rich decision support material for industry practitioners
seeking to adopt new technologies and researchers looking for an empirical basis on which to build new or refined technologies.
This paper presents model for evaluating the rigor and industrial relevance of technology evaluations in software engineering.
The model is applied and validated in a comprehensive systematic literature review of evaluations of requirements engineering
technologies published in software engineering journals. The aim is to show the applicability of the model and to characterize
how evaluations are carried out and reported to evaluate the state-of-research. The review shows that the model can be applied
to characterize evaluations in requirements engineering. The findings from applying the model also show that the majority
of technology evaluations in requirements engineering lack both industrial relevance and rigor. In addition, the research
field does not show any improvements in terms of industrial relevance over time.},
 author = {Martin Ivarsson and Tony Gorschek},
 doi = {10.1007/s10664-010-9146-4},
 journal = {Empirical Software Engineering},
 month = {oct},
 number = {3},
 pages = {365--395},
 publisher = {Springer Science and Business Media {LLC}},
 title = {A method for evaluating rigor and industrial relevance of technology evaluations},
 url = {https://doi.org/10.1007%2Fs10664-010-9146-4},
 volume = {16},
 year = {2010}
}

@article{Kasoju_2013,
 abstract = {},
 author = {Abhinaya Kasoju and Kai Petersen and Mika V. Mäntylä},
 doi = {10.1016/j.infsof.2013.01.005},
 journal = {Information and Software Technology},
 month = {jul},
 number = {7},
 pages = {1237--1259},
 publisher = {Elsevier {BV}},
 title = {Analyzing an automotive testing process with evidence-based software engineering},
 url = {https://doi.org/10.1016%2Fj.infsof.2013.01.005},
 volume = {55},
 year = {2013}
}

@inproceedings{Kitchenham,
 abstract = {Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.},
 author = {B.A. Kitchenham and T. Dyba and M. Jorgensen},
 booktitle = {Proceedings. 26th International Conference on Software Engineering},
 doi = {10.1109/icse.2004.1317449},
 publisher = {{IEEE} Comput. Soc},
 title = {Evidence-based software engineering},
 url = {https://doi.org/10.1109%2Ficse.2004.1317449}
}

@article{Knottnerus_2010,
 abstract = {},
 author = {J. Andr{\'{e}} Knottnerus and Peter Tugwell},
 doi = {10.1016/j.jclinepi.2010.08.001},
 journal = {Journal of Clinical Epidemiology},
 month = {oct},
 number = {10},
 pages = {1051--1052},
 publisher = {Elsevier {BV}},
 title = {Real world research},
 url = {https://doi.org/10.1016%2Fj.jclinepi.2010.08.001},
 volume = {63},
 year = {2010}
}

@misc{Moore_2002,
 abstract = {Identifying an agreed body of knowledge is an essential step in moving software engineering from an ideal to a recognized profession. Sponsored by the IEEE Computer Society and managed by the Universite du Quebec a Montreal, the Software Engineering Body of Knowledge (SWEBOK) project is developing a guide to the body of knowledge of software engineering. A trial version of the guide is currently available without charge at http://www.swebok.org. Following an additional two years of trial usage and feedback, the project will revise the Guide.

This article will begin by discussing the desired characteristics of a profession for software engineering. Then the objectives and contents of the Guide are described. Finally, the SWEBOK project and its future will be described.

Keywords:

software engineering profession;
organization;
SWEBOK guide;
overview;
SWEBOK project},
 author = {James W. Moore and Pierre Bourque and Robert Dupuis and Alain Abran and Leonard Tripp},
 doi = {10.1002/0471028959.sof312},
 month = {jan},
 publisher = {John Wiley {\&} Sons, Inc.},
 title = {Software Engineering Body of Knowledge Project},
 url = {https://doi.org/10.1002%2F0471028959.sof312},
 year = {2002}
}

@article{Munir_2014,
 abstract = {},
 author = {Hussan Munir and Misagh Moayyed and Kai Petersen},
 doi = {10.1016/j.infsof.2014.01.002},
 journal = {Information and Software Technology},
 month = {apr},
 number = {4},
 pages = {375--394},
 publisher = {Elsevier {BV}},
 title = {Considering rigor and relevance when evaluating test driven development: A systematic review},
 url = {https://doi.org/10.1016%2Fj.infsof.2014.01.002},
 volume = {56},
 year = {2014}
}

@inproceedings{Petersen_2009,
 abstract = {},
 author = {Kai Petersen and Claes Wohlin},
 booktitle = {2009 3rd International Symposium on Empirical Software Engineering and Measurement},
 doi = {10.1109/esem.2009.5316010},
 month = {oct},
 publisher = {{IEEE}},
 title = {Context in industrial software engineering research},
 url = {https://doi.org/10.1109%2Fesem.2009.5316010},
 year = {2009}
}

@article{Rafique_2013,
 abstract = {This paper provides a systematic meta-analysis of 27 studies that investigate the impact of Test-Driven Development (TDD) on external code quality and productivity. The results indicate that, in general, TDD has a small positive effect on quality but little to no discernible effect on productivity. However, subgroup analysis has found both the quality improvement and the productivity drop to be much larger in industrial studies in comparison with academic studies. A larger drop of productivity was found in studies where the difference in test effort between the TDD and the control group's process was significant. A larger improvement in quality was also found in the academic studies when the difference in test effort is substantial; however, no conclusion could be derived regarding the industrial studies due to the lack of data. Finally, the influence of developer experience and task size as moderator variables was investigated, and a statistically significant positive correlation was found between task size and the magnitude of the improvement in quality.},
 author = {Yahya Rafique and Vojislav B. Misic},
 doi = {10.1109/tse.2012.28},
 journal = {{IEEE} Transactions on Software Engineering},
 month = {jun},
 number = {6},
 pages = {835--856},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {The Effects of Test-Driven Development on External Quality and Productivity: A Meta-Analysis},
 url = {https://doi.org/10.1109%2Ftse.2012.28},
 volume = {39},
 year = {2013}
}

@inproceedings{Rainer_2005,
 abstract = {},
 author = {Austen Rainer and Dorota Jagielska and Tracy Hall},
 booktitle = {Proceedings of the 2005 workshop on Realising evidence-based software engineering  - {REBSE} {\textquotesingle}05},
 doi = {10.1145/1083174.1083177},
 publisher = {{ACM} Press},
 title = {Software engineering practice versus evidence-based software engineering research},
 url = {https://doi.org/10.1145%2F1083174.1083177},
 year = {2005}
}

@article{Runeson_2008,
 abstract = {Abstract,Case study is a suitable research methodology,for software engineering,research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims,at providing,an introduction to case study methodology,and,guidelines for researchers,conducting,case studies and,readers studying,reports of such,studies. The content is based on the authors’ own,experience from conducting,and reading case studies. The terminology,and,guidelines are compiled,from,different methodology,handbooks,in other research domains, in particular social science and information systems, and adapted to the needs,in software,engineering. We,present,recommended,practices for software engineering,case studies as well,as empirically,derived,and,evaluated,checklists for researchers and readers of case study research. Keywords,Casestudy.Research methodology.Checklists .Guidelines},
 author = {Per Runeson and Martin Höst},
 doi = {10.1007/s10664-008-9102-8},
 journal = {Empirical Software Engineering},
 month = {dec},
 number = {2},
 pages = {131--164},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Guidelines for conducting and reporting case study research in software engineering},
 url = {https://doi.org/10.1007%2Fs10664-008-9102-8},
 volume = {14},
 year = {2008}
}

@article{Svahnberg_2010,
 abstract = {Context: Strategic release planning (sometimes referred to as road-mapping) is an important phase of the requirements engineering process performed at product level. It is concerned with selection and assignment of requirements in sequences of releases such that important technical and resource constraints are fulfilled. Objectives: In this study we investigate which strategic release planning models have been proposed, their degree of empirical validation, their factors for requirements selection, and whether they are intended for a bespoke or market-driven requirements engineering context. Methods: In this systematic review a number of article sources are used, including Compendex, Inspec, IEEE Xplore, ACM Digital Library, and Springer Link. Studies are selected after reading titles and abstracts to decide whether the articles are peer reviewed, and relevant to the subject. Results: Twenty four strategic release planning models are found and mapped in relation to each other, and a taxonomy of requirements selection factors is constructed. Conclusions: We conclude that many models are related to each other and use similar techniques to address the release planning problem. We also conclude that several requirement selection factors are covered in the different models, but that many methods fail to address factors such as stakeholder value or internal value. Moreover, we conclude that there is a need for further empirical validation of the models in full scale industry trials.},
 author = {Mikael Svahnberg and Tony Gorschek and Robert Feldt and Richard Torkar and Saad Bin Saleem and Muhammad Usman Shafique},
 doi = {10.1016/j.infsof.2009.11.006},
 journal = {Information and Software Technology},
 month = {mar},
 number = {3},
 pages = {237--248},
 publisher = {Elsevier {BV}},
 title = {A systematic review on strategic release planning models},
 url = {https://doi.org/10.1016%2Fj.infsof.2009.11.006},
 volume = {52},
 year = {2010}
}

@article{Unterkalmsteiner_2014,
 abstract = {Requirements Engineering and Software Testing are mature areas and have seen a lot of research. Nevertheless, their interactions have been sparsely explored beyond the concept of traceability. To fill this gap we propose a definition of requirements engineering and software test (REST) alignment, a taxonomy that characterizes the methods linking the respective areas, and a process to assess alignment. The taxonomy can support researchers to identify new opportunities for investigation, as well as practitioners to compare alignment methods and evaluate alignment, or lack thereof.
We constructed the REST taxonomy by analyzing alignment methods published in literature, iteratively validating the emerging dimensions. The resulting concept of an information dyad characterizes the exchange of information required for any alignment to take place. We demonstrate use of the taxonomy by applying it on five in-depth cases
and illustrate angles of analysis on a set of thirteen alignment methods. In addition we developed an assessment framework (REST-bench), applied it in an industrial assessment, and showed that it, with a low effort, can identify opportunities to improve REST alignment.
Although we expect that the taxonomy can be further refined, we believe that the information dyad is a valid and useful construct to understand alignment.},
 author = {M. Unterkalmsteiner and R. Feldt and T. Gorschek},
 doi = {10.1145/2523088},
 journal = {{ACM} Transactions on Software Engineering and Methodology},
 month = {mar},
 number = {2},
 pages = {1--38},
 publisher = {Association for Computing Machinery ({ACM})},
 title = {A taxonomy for requirements engineering and software test alignment},
 url = {https://doi.org/10.1145%2F2523088},
 volume = {23},
 year = {2014}
}

@inproceedings{Walcott_2006,
 abstract = {Regression test prioritization is often performed in a time constrained execution environment in which testing only oc-curs for a xed time period. For example, many organiza-tions rely upon nightly building and regression testing of their applications every time source code changes are com-mitted to a version control repository. This paper presents a regression test prioritization technique that uses a genetic algorithm to reorder test suites in light of testing time con-straints. Experiment results indicate that our prioritiza-tion approach frequently yields higher average percentage of faults detected (APFD) values, for two case study appli-cations, when basic block level coverage is used instead of method level coverage. The experiments also reveal funda-mental trade-o s in the performance of time-aware prioriti-zation. This paper shows that our prioritization technique is appropriate for many regression testing environments and explains how the baseline approach can be extended to op-erate in additional time constrained testing circumstances.},
 author = {Kristen R. Walcott and Mary Lou Soffa and Gregory M. Kapfhammer and Robert S. Roos},
 booktitle = {Proceedings of the 2006 international symposium on Software testing and analysis},
 doi = {10.1145/1146238.1146240},
 month = {jul},
 publisher = {{ACM}},
 title = {{TimeAware} test suite prioritization},
 url = {https://doi.org/10.1145%2F1146238.1146240},
 year = {2006}
}

@article{Wohlin_2012,
 abstract = {Collaboration between industry and academia supports improvement and innovation in industry and helps to ensure industrial relevance in academic research. This article presents an exploratory study of factors for successful collaboration between industry and academia. A survey designed for data collection was first conducted in Sweden and then replicated in Australia. The context for the two studies is different, thus forming a starting point for potential generalizations in the future: the industrial side of collaboration is the key element for successful collaboration, with key factors being buy-in and support from company management and collaboration champion on site. These findings may help industry and academia to set up successful collaborative ventures in the future},
 author = {Claes Wohlin and Aybuke Aurum and Lefteris Angelis and Laura Phillips and Yvonne Dittrich and Tony Gorschek and Hakan Grahn and Kennet Henningsson and Simon Kagstrom and Graham Low and Per Rovegard and Piotr Tomaszewski and Christine van Toorn and Jeff Winter},
 doi = {10.1109/ms.2011.92},
 journal = {{IEEE} Software},
 month = {mar},
 number = {2},
 pages = {67--73},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {The Success Factors Powering Industry-Academia Collaboration},
 url = {https://doi.org/10.1109%2Fms.2011.92},
 volume = {29},
 year = {2012}
}

@book{1995,
 abstract = {},
 doi = {10.1007/3-540-58867-1},
 editor = {Claus Lewerentz and Thomas Lindner},
 publisher = {Springer Berlin Heidelberg},
 title = {Formal Development of Reactive Systems},
 url = {https://doi.org/10.1007%2F3-540-58867-1},
 year = {1995}
}

@book{2009,
 abstract = {This book constitutes the refereed proceedings of the 5th European Conference on Model Driven Architecture - Foundations and Applications, ECMDA-FA 2009, held in Enschede, The Netherlands, in June 2009.
The 23 revised full papers presented - 16 research papers and 7 industry papers - were carefully reviewed and selected from 72 submissions. Promoting the use of models in the specification, design, analysis, synthesis, deployment, and evolution of complex software systems, the papers address all current issues of model-driven architecture including model transformations, modelling language issues, modelling of behaviour and time, traceability and scalability, model-based embedded systems engineering, and the application of model-driven development to IT and networking systems.},
 doi = {10.1007/978-3-642-02674-4},
 editor = {Richard F. Paige and Alan Hartman and Arend Rensink},
 publisher = {Springer Berlin Heidelberg},
 title = {Model Driven Architecture - Foundations and Applications},
 url = {https://doi.org/10.1007%2F978-3-642-02674-4},
 year = {2009}
}

@incollection{Behjati_2011,
 abstract = {Recent years have seen a proliferation of languages for describing embedded systems. Some of these languages have emerged
from domain-specific frameworks, and some are adaptions or extensions of more general-purpose languages. In this paper, we
focus on two widely-used standard languages: the Architecture Analysis and Design Language (AADL) and the Systems Modeling
Language (SysML). AADL was born as an avionics-focused domain-specific language and later on has been revised to represent
and support a more general category of embedded real-time systems. SysML is an extension of the Unified Modeling Language 
(UML) intended to support system engineering and modeling. We propose the ExSAM profile that extends SysML by adding AADL
concepts to it with the goal of exploiting the key advantages of both languages in a seamless way. More precisely, by using
ExSAM and any SysML modeling environment, we will be able to both model system engineering concepts and use AADL analysis
tools where needed. We describe the ExSAM profile through several examples and compare it with existing alternatives. We have
implemented ExSAM using IBM Rational Rhapsody and evaluated its completeness and usefulness through two case studies.},
 author = {Razieh Behjati and Tao Yue and Shiva Nejati and Lionel Briand and Bran Selic},
 booktitle = {Modelling Foundations and Applications},
 doi = {10.1007/978-3-642-21470-7_17},
 pages = {236--252},
 publisher = {Springer Berlin Heidelberg},
 title = {Extending {SysML} with {AADL} Concepts for Comprehensive System Architecture Modeling},
 url = {https://doi.org/10.1007%2F978-3-642-21470-7_17},
 year = {2011}
}

@incollection{Behjati_2012,
 abstract = {Configuring Integrated Control Systems (ICSs) is largely manual, time-consuming and error-prone. In this paper, we propose a model-based configuration approach that interactively guides engineers to configure software embedded in ICSs. Our approach verifies engineers' decisions at each configuration iteration, and further, automates some of the decisions. We use a constraint solver, SICStus Prolog, to automatically infer configuration decisions and to ensure the consistency of configuration data. We evaluated our approach by applying it to a real subsea oil production system. Specifically, we rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach successfully enforces consistency of configurations, can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
 author = {Razieh Behjati and Shiva Nejati and Tao Yue and Arnaud Gotlieb and Lionel Briand},
 booktitle = {Modelling Foundations and Applications},
 doi = {10.1007/978-3-642-31491-9_18},
 pages = {226--243},
 publisher = {Springer Berlin Heidelberg},
 title = {Model-Based Automated and Guided Configuration of Embedded Software Systems},
 url = {https://doi.org/10.1007%2F978-3-642-31491-9_18},
 year = {2012}
}

@article{Behjati_2013,
 abstract = {},
 author = {Razieh Behjati and Tao Yue and Lionel Briand and Bran Selic},
 doi = {10.1016/j.infsof.2012.09.006},
 journal = {Information and Software Technology},
 month = {mar},
 number = {3},
 pages = {607--629},
 publisher = {Elsevier {BV}},
 title = {{SimPL}: A product-line modeling methodology for families of integrated control systems},
 url = {https://doi.org/10.1016%2Fj.infsof.2012.09.006},
 volume = {55},
 year = {2013}
}

@article{Briand_2014,
 abstract = {Certifying safety-critical software and ensuring its safety requires checking the conformance between safety requirements and design. Increasingly, the development of safety-critical software relies on modeling, and the System Modeling Language (SysML) is now commonly used in many industry sectors. Inspecting safety conformance by comparing design models against safety requirements requires safety inspectors to browse through large models and is consequently time consuming and error-prone. To address this, we have devised a mechanism to establish traceability between (functional) safety requirements and SysML design models to extract design slices (model fragments) that filter out irrelevant details but keep enough context information for the slices to be easy to inspect and understand. In this article, we report on a controlled experiment assessing the impact of the traceability and slicing mechanism on inspectors' conformance decisions and effort. Results show a significant decrease in effort and an increase in decisions' correctness and level of certainty.},
 author = {Lionel Briand and Davide Falessi and Shiva Nejati and Mehrdad Sabetzadeh and Tao Yue},
 doi = {10.1145/2559978},
 journal = {{ACM} Transactions on Software Engineering and Methodology},
 month = {feb},
 number = {1},
 pages = {1--43},
 publisher = {Association for Computing Machinery ({ACM})},
 title = {Traceability and {SysML} design slices to support safety inspections},
 url = {https://doi.org/10.1145%2F2559978},
 volume = {23},
 year = {2014}
}

@book{Ericson_2005,
 abstract = {},
 author = {Clifton A. Ericson},
 doi = {10.1002/0471739421},
 month = {jul},
 publisher = {John Wiley {\&} Sons, Inc.},
 title = {Hazard Analysis Techniques for System Safety},
 url = {https://doi.org/10.1002%2F0471739421},
 year = {2005}
}

@inproceedings{Falessi_2011,
 abstract = {Software safety certification involves checking that the software design meets the (software) safety requirements. In practice, inspections are one of the primary vehicles for ensuring that safety requirements are satisfied by the design. Unless the safety-related aspects of the design are clearly delineated, the inspections conducted by safety assessors would have to consider the entire design, although only small fragments of the design may be related to safety. In a model-driven development context, this means that the assessors have to browse through large models, understand them, and identify the safety-related fragments. This is time-consuming and error-prone, specially noting that the assessors are often third-party regulatory bodies who were not involved in the design. To address this problem, we describe in this paper a prototype tool called, SafeSlice, that enables one to automatically extract the safety-related slices (fragments) of design models. The main enabler for our slicing technique is the traceability between the safety requirements and the design, established by following a structured design methodology that we propose. Our work is grounded on SysML, which is being increasingly used for expressing the design of safety-critical systems. We have validated our work through two case studies and a control experiment which we briefly outline in the paper.},
 author = {Davide Falessi and Shiva Nejati and Mehrdad Sabetzadeh and Lionel Briand and Antonio Messina},
 booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering},
 doi = {10.1145/2025113.2025191},
 month = {sep},
 publisher = {{ACM}},
 title = {{SafeSlice}},
 url = {https://doi.org/10.1145%2F2025113.2025191},
 year = {2011}
}

@inproceedings{France_2007,
 abstract = {The term Model-Driven Engineering (MDE) is typically used to describe software development approaches in which abstract models of software systems are created and systematically transformed to concrete implementations. In this paper we give an overview of current research in MDE and discuss some of the major challenges that must be tackled in order to realize the MDE vision of software development. We argue that full realizations of the MDE vision may not be possible in the near to medium-term primarily because of the wicked problems involved. On the other hand, attempting to realize the vision will provide insights that can be used to significantly reduce the gap between evolving software complexity and the technologies used to manage complexity.},
 author = {Robert France and Bernhard Rumpe},
 booktitle = {Future of Software Engineering ({FOSE} {\textquotesingle}07)},
 doi = {10.1109/fose.2007.14},
 month = {may},
 publisher = {{IEEE}},
 title = {Model-driven Development of Complex Software: A Research Roadmap},
 url = {https://doi.org/10.1109%2Ffose.2007.14},
 year = {2007}
}

@article{Gorschek_2006,
 abstract = {Technology transfer, and thus industry-relevant research, involves more than merely producing research results and delivering them in publications and technical reports. It demands close cooperation and collaboration between industry and academia throughout the entire research process. During research conducted in a partnership between Blekinge Institute of Technology and two companies, Danaher Motion Saro AB (DHR) and ABB, we devised a technology transfer model that embodies this philosophy. We initiated this partnership to conduct industry-relevant research in requirements engineering and product management. Technology transfer in this context is a prerequisite: it validates academic research results in a real setting, and it provides a way to improve industry development and business processes},
 author = {Tony Gorschek and Per Garre and Stig Larsson and Claes Wohlin},
 doi = {10.1109/ms.2006.147},
 journal = {{IEEE} Software},
 month = {nov},
 number = {6},
 pages = {88--95},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {A Model for Technology Transfer in Practice},
 url = {https://doi.org/10.1109%2Fms.2006.147},
 volume = {23},
 year = {2006}
}

@inproceedings{Hutchinson_2011,
 abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some ‘lessons learned’, in particular the importance of complex organizational, managerial and social factors – as opposed to simple technical factors – in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
 author = {John Hutchinson and Mark Rouncefield and Jon Whittle},
 booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
 doi = {10.1145/1985793.1985882},
 month = {may},
 publisher = {{ACM}},
 title = {Model-driven engineering practices in industry},
 url = {https://doi.org/10.1145%2F1985793.1985882},
 year = {2011}
}

@inproceedings{Hutchinson_2011,
 abstract = {This paper presents some initial results from a twelve-month empirical research study of model driven engineering (MDE). Using largely qualitative questionnaire and interview methods we investigate and document a range of technical, organizational and social factors that apparently influence organizational responses to MDE: specifically, its perception as a successful or unsuccessful organizational intervention. We then outline a range of lessons learned. Whilst, as with all qualitative research, these lessons should be interpreted with care, they should also be seen as providing a greater understanding of MDE practice in industry, as well as shedding light on the varied, and occasionally surprising, social, technical and organizational factors that affect success and failure. We conclude by suggesting how the next phase of the research will attempt to investigate some of these issues from a different angle and in greater depth.},
 author = {John Hutchinson and Jon Whittle and Mark Rouncefield and Steinar Kristoffersen},
 booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
 doi = {10.1145/1985793.1985858},
 month = {may},
 publisher = {{ACM}},
 title = {Empirical assessment of {MDE} in industry},
 url = {https://doi.org/10.1145%2F1985793.1985858},
 year = {2011}
}

@article{Kim_2000,
 abstract = {},
 author = {Hoon Kim},
 doi = {10.1080/00401706.2000.10485722},
 journal = {Technometrics},
 month = {nov},
 number = {4},
 pages = {430--431},
 publisher = {Informa {UK} Limited},
 title = {Monte Carlo Statistical Methods},
 url = {https://doi.org/10.1080%2F00401706.2000.10485722},
 volume = {42},
 year = {2000}
}

@incollection{Mohagheghi,
 abstract = {Model-Driven Engineering (MDE) has been promoted as a solution to handle the complexity of software development by raising
the abstraction level and automating labor-intensive and error-prone tasks. However, few efforts have been made at collecting
evidence to evaluate its benefits and limitations, which is the subject of this review. We searched several publication channels
in the period 2000 to June 2007 for empirical studies on applying MDE in industry, which produced 25 papers for the review.
Our findings include industry motivations for investigating MDE and the different domains it has been applied to. In most
cases the maturity of third-party tool environments is still perceived as unsatisfactory for large-scale industrial adoption.
We found reports of improvements in software quality and of both productivity gains and losses, but these reports were mainly
from small-scale studies. There are a few reports on advantages of applying MDE in larger projects, however, more empirical
studies and detailed data are needed to strengthen the evidence. We conclude that there is too little evidence to allow generalization
of the results at this stage.},
 author = {Parastoo Mohagheghi and Vegard Dehlen},
 booktitle = {Model Driven Architecture {\textendash} Foundations and Applications},
 doi = {10.1007/978-3-540-69100-6_31},
 pages = {432--443},
 publisher = {Springer Berlin Heidelberg},
 title = {Where Is the Proof? - A Review of Experiences from Applying {MDE} in Industry},
 url = {https://doi.org/10.1007%2F978-3-540-69100-6_31}
}

@article{Nejati_2012,
 abstract = {},
 author = {Shiva Nejati and Mehrdad Sabetzadeh and Davide Falessi and Lionel Briand and Thierry Coq},
 doi = {10.1016/j.infsof.2012.01.005},
 journal = {Information and Software Technology},
 month = {jun},
 number = {6},
 pages = {569--590},
 publisher = {Elsevier {BV}},
 title = {A {SysML}-based approach to traceability management and design slicing in support of safety certification: Framework, tool support, and case studies},
 url = {https://doi.org/10.1016%2Fj.infsof.2012.01.005},
 volume = {54},
 year = {2012}
}

@book{O_Hagan_2006,
 abstract = {Elicitation is the process of extracting expert knowledge about some unknown quantity or quantities, and formulating that information as a probability distribution. Elicitation is important in situations, such as modelling the safety of nuclear installations or assessing the risk of terrorist attacks, where expert knowledge is essentially the only source of good information. It also plays a major role in other contexts by augmenting scarce observational data, through the use of Bayesian statistical methods. However, elicitation is not a simple task, and practitioners need to be aware of a wide range of research findings in order to elicit expert judgements accurately and reliably. Uncertain Judgements introduces the area, before guiding the reader through the study of appropriate elicitation methods, illustrated by a variety of multi-disciplinary examples.},
 author = {Anthony O{\textquotesingle}Hagan and Caitlin E. Buck and Alireza Daneshkhah and J. Richard Eiser and Paul H. Garthwaite and David J. Jenkinson and Jeremy E. Oakley and Tim Rakow},
 doi = {10.1002/0470033312},
 month = {jul},
 publisher = {Wiley},
 title = {Uncertain Judgements: Eliciting Experts{\textquotesingle} Probabilities},
 url = {https://doi.org/10.1002%2F0470033312},
 year = {2006}
}

@inproceedings{Sabetzadeh_2011,
 abstract = {New technologies typically involve innovative aspects that are not addressed by the existing normative standards and hence are not assessable through common certification procedures. To ensure that new technologies can be implemented in a safe and reliable manner, a specific kind of assessment is performed, which in many industries, e.g., the energy sector, is known as Technology Qualification (TQ). TQ aims at demonstrating with an acceptable level of confidence that a new technology will function within specified limits. Expert opinion plays an important role in TQ, both to identify the safety and reliability evidence that needs to be developed, and to interpret the evidence provided. Hence, it is crucial to apply a systematic process for eliciting expert opinions, and to use the opinions for measuring the satisfaction of a technology's safety and reliability objectives. In this paper, drawing on the concept of assurance cases, we propose a goal-based approach for TQ. The approach, which is supported by a software tool, enables analysts to quantitatively reason about the satisfaction of a technology's overall goals and further to identify the aspects that must be improved to increase goal satisfaction. The three main components enabling quantitative assessment are goal models, expert elicitation, and probabilistic simulation. We report on an industrial pilot study where we apply our approach for assessing a new offshore technology.},
 author = {Mehrdad Sabetzadeh and Davide Falessi and Lionel Briand and Stefano Di Alesio and Dag McGeorge and Vidar {\AA}hjem and Jonas Borg},
 booktitle = {2011 {IEEE} 13th International Symposium on High-Assurance Systems Engineering},
 doi = {10.1109/hase.2011.22},
 month = {nov},
 publisher = {{IEEE}},
 title = {Combining Goal Models, Expert Elicitation, and Probabilistic Simulation for Qualification of New Technology},
 url = {https://doi.org/10.1109%2Fhase.2011.22},
 year = {2011}
}

@inproceedings{Sabetzadeh_2011,
 abstract = {Safety-critical embedded systems often need to undergo a rigorous certification process to ensure that the safety risks associated with the use of the systems are adequately mitigated. Interfaces between software and hardware components (SW/HW interfaces) play a fundamental role in these systems by linking the systems' control software to either the physical hardware components or to a hardware abstraction layer. Subsequently, safety certification of embedded systems necessarily has to cover the SW/HW interfaces used in these systems. In this paper, we describe a Model Driven Engineering (MDE) approach based on the SysML language, targeted at facilitating the certification of SW/HW interfaces in embedded systems. Our work draws on our experience with maritime and energy systems, but the work should also apply to a broader set of domains, e.g., the automotive sector, where similar design principles are used for (SW/HW) interface design. Our approach leverages our previous work on the development of SysML-based modeling and analysis techniques for safety-critical systems. Specifically, we tailor the methodology developed in our previous work to the development of safety-critical interfaces, and provide step-by-step and practical guidelines aimed at providing the evidence necessary for arguing that the safety-related requirements of an interface are properly addressed by its design. We describe an application of our proposed guidelines to a representative safety-critical interface in the maritime and energy domain.},
 author = {Mehrdad Sabetzadeh and Shiva Nejati and Lionel Briand and Anne-Heidi Evensen Mills},
 booktitle = {2011 {IEEE} 13th International Symposium on High-Assurance Systems Engineering},
 doi = {10.1109/hase.2011.23},
 month = {nov},
 publisher = {{IEEE}},
 title = {Using {SysML} for Modeling of Safety-Critical Software-Hardware Interfaces: Guidelines and Industry Experience},
 url = {https://doi.org/10.1109%2Fhase.2011.23},
 year = {2011}
}

@article{Sabetzadeh_2013,
 abstract = {New technologies typically involve innovative aspects that are not addressed by the existing normative standards and hence are not assessable through common certification procedures. To ensure that new technologies can be implemented in a safe and reliable manner, a specific kind of assessment is performed, which in many industries, e.g., the energy sector, is known as Technology Qualification (TQ). TQ aims at demonstrating with an acceptable level of confidence that a new technology will function within specified limits. Expert opinion plays an important role in TQ, both to identify the safety and reliability evidence that needs to be developed and to interpret the evidence provided. Since there are often multiple experts involved in TQ, it is crucial to apply a structured process for eliciting expert opinions, and to use this information systematically when analyzing the satisfaction of the technology's safety and reliability objectives.In this paper, we present a goal-based approach for TQ. Our approach enables analysts to quantitatively reason about the satisfaction of the technology's overall goals and further to identify the aspects that must be improved to increase goal satisfaction. The approach is founded on three main components: goal models, expert elicitation, and probabilistic simulation. We describe a tool, named Modus, that we have developed in support of our approach. We provide an extensive empirical validation of our approach through two industrial case studies and a survey.},
 author = {Mehrdad Sabetzadeh and Davide Falessi and Lionel Briand and Stefano Di Alesio},
 doi = {10.1016/j.ress.2013.05.005},
 journal = {Reliability Engineering {\&}amp$\mathsemicolon$ System Safety},
 month = {nov},
 pages = {52--66},
 publisher = {Elsevier {BV}},
 title = {A goal-based approach for qualification of new technologies: Foundations, tool support, and industrial validation},
 url = {https://doi.org/10.1016%2Fj.ress.2013.05.005},
 volume = {119},
 year = {2013}
}

@inproceedings{Berniker,
 abstract = {Three underlying models of technology transfer emerged from an
analysis of interviews with 55 aerospace managers using a dialectic
method. One called for the transfer of experts, another for better
communication, and the third required the technologies be on the shelf.
The models were based on conflicting assumptions and were
self-contradictory when pushed to an extreme. Together, they encompassed
critical issues of technology transfer. A composite team organization
was proposed which successfully met project, R&D, and technology
transfer requirements},
 author = {E. Berniker},
 booktitle = {Technology Management : the New International Language},
 doi = {10.1109/picmet.1991.183700},
 publisher = {{IEEE}},
 title = {Models of technology transfer. (A dialectical case study)},
 url = {https://doi.org/10.1109%2Fpicmet.1991.183700}
}

@article{Bozeman_2000,
 abstract = {My purpose is to review, synthesize and criticize the voluminous, multidisciplinary literature on technology transfer. To reduce the literature to manageable proportions, I focus chiefly (not exclusively) on recent literature on domestic technology transfer from universities and government laboratories. I begin by examining a set of fundamental conceptual issues, especially the ways in which the analytical ambiguities surrounding technology transfer concepts affect research and theory. My literature review follows and I emphasize technology transfer's impact and effectiveness. I employ a “Contingent Effectiveness Model of Technology Transfer” to organize the literature. As the model's name implies, it assumes that technology effectiveness can take a variety of forms. In addition to examining the more traditional effectiveness criteria- those rooted in market impacts- the model considers a number of alternative effectiveness criteria, including political effectiveness, capacity-building.},
 author = {Barry Bozeman},
 doi = {10.1016/s0048-7333(99)00093-1},
 journal = {Research Policy},
 month = {apr},
 number = {4-5},
 pages = {627--655},
 publisher = {Elsevier {BV}},
 title = {Technology transfer and public policy: a review of research and theory},
 url = {https://doi.org/10.1016%2Fs0048-7333%2899%2900093-1},
 volume = {29},
 year = {2000}
}

@article{Bozeman_2012,
 abstract = {There is abundant evidence that research collaboration has become the norm in every field of scientific and technical research. We provide a critical overview of the literature on research collaboration, focusing particularly on individual-level collaborations among university researchers, but we also give attention to university researchers’ collaborations with researchers in other sectors, including industry. We consider collaborations aimed chiefly at expanding the base of knowledge (knowledge-focused collaborations) as well as ones focused on production of economic value and wealth (property-focused collaborations), the latter including most academic entrepreneurship research collaborations. To help organize our review we develop a framework for analysis, one that considers attributes of collaborators, collaborative process and organization characteristics as the affect collaboration choices and outcomes. In addition, we develop and use a “Propositional Table for Research Collaboration Literature,” presented as an “Appendix” to this study. We conclude with some suggestions for possible improvement in research on collaboration including: (1) more attention to multiple levels of analysis and the interactions among them; (2) more careful measurement of impacts as opposed to outputs; (3) more studies on ‘malpractice’ in collaboration, including exploitation; (4) increased attention to collaborators’ motives and the social psychology of collaborative teams.},
 author = {Barry Bozeman and Daniel Fay and Catherine P. Slade},
 doi = {10.1007/s10961-012-9281-8},
 journal = {The Journal of Technology Transfer},
 month = {nov},
 number = {1},
 pages = {1--67},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Research collaboration in universities and academic entrepreneurship: the-state-of-the-art},
 url = {https://doi.org/10.1007%2Fs10961-012-9281-8},
 volume = {38},
 year = {2012}
}

@inproceedings{Diebold_2014,
 abstract = {Background: The mobility domains are moving towards the adoption of multicore technology. Appropriate methods, techniques, and tools need to be developed or adapted in order to fulfill the existing requirements. This is a case for design space exploration methods and tools. Objective: Our goal was to understand the importance of different design space exploration goals with respect to their relevance, frequency of use, and tool support required in the development of multicore systems from the point of view of the ARAMiS project members. Our aim was to use the results to guide further work in the project. Method: We conducted a survey regarding the current state of the art in design space exploration in industry and research and collected the expectations of project members regarding design space exploration goals. Results: The results show that design space exploration is an important topic in industry as well as in research. It is used very often with different important goals to optimize the system. Conclusions: Current tools provide only partial solutions for design space exploration. Our results can be used for improving them and guiding their development according to the priorities explained in this contribution.},
 author = {Philipp Diebold and Constanza Lampasona and Sergey Zverlov and Sebastian Voss},
 booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
 doi = {10.1145/2601248.2601250},
 month = {may},
 publisher = {{ACM}},
 title = {Practitioners{\textquotesingle} and researchers{\textquotesingle} expectations on design space exploration for multicore systems in the automotive and avionics domains},
 url = {https://doi.org/10.1145%2F2601248.2601250},
 year = {2014}
}

@article{Gorschek_2006,
 abstract = {Technology transfer, and thus industry-relevant research, involves more than merely producing research results and delivering them in publications and technical reports. It demands close cooperation and collaboration between industry and academia throughout the entire research process. During research conducted in a partnership between Blekinge Institute of Technology and two companies, Danaher Motion Saro AB (DHR) and ABB, we devised a technology transfer model that embodies this philosophy. We initiated this partnership to conduct industry-relevant research in requirements engineering and product management. Technology transfer in this context is a prerequisite: it validates academic research results in a real setting, and it provides a way to improve industry development and business processes},
 author = {Tony Gorschek and Per Garre and Stig Larsson and Claes Wohlin},
 doi = {10.1109/ms.2006.147},
 journal = {{IEEE} Software},
 month = {nov},
 number = {6},
 pages = {88--95},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {A Model for Technology Transfer in Practice},
 url = {https://doi.org/10.1109%2Fms.2006.147},
 volume = {23},
 year = {2006}
}

@article{Pfleeger_1999,
 abstract = {Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.},
 author = {S.L Pfleeger},
 doi = {10.1016/s0164-1212(99)00031-x},
 journal = {Journal of Systems and Software},
 month = {jul},
 number = {2-3},
 pages = {111--124},
 publisher = {Elsevier {BV}},
 title = {Understanding and improving technology transfer in software engineering},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900031-x},
 volume = {47},
 year = {1999}
}

@article{REISMAN_2005,
 abstract = {Transfer of technologies (TT) takes place among various kinds of players, takes on various kinds of modalities and is done for various motivations. Its literature is very disjoint and disparate. It transcends several academic disciplines and professions. This paper presents a taxonomy defining the field in its entirety and delineating all of its facets in a manner that is parsimonious yet discriminating. Many potential uses for the taxonomy are identified. These include more effective teaching of TT subject matter.},
 author = {A REISMAN},
 doi = {10.1016/j.omega.2004.04.004},
 journal = {Omega},
 month = {jun},
 number = {3},
 pages = {189--202},
 publisher = {Elsevier {BV}},
 title = {Transfer of technologies: a cross-disciplinary taxonomy},
 url = {https://doi.org/10.1016%2Fj.omega.2004.04.004},
 volume = {33},
 year = {2005}
}

@inproceedings{Rombach_2007,
 abstract = {The rapid and complex research and innovation processes require high-tech companies to optimize their technology transfer processes. It is clearly not sufficient to solely rely on internal R&D; strategic cooperations with external research centers of excellence are needed in order to compete in the global innovation market. Candidates for such strategic cooperations are universities, research institutions, and technology focused consulting companies. Key challenge is the effective integration of external competences into the company-internal innovation processes. In this paper we present a survey of the state-of-the-art in technology transfer, high-light promising success cases for the future, and derive success criteria for successful technology transfer in a global world. The cooperation between Siemens and Fraunhofer IESE is presented as a concrete example.},
 author = {Dieter Rombach and Reinhold Achatz},
 booktitle = {Future of Software Engineering ({FOSE} {\textquotesingle}07)},
 doi = {10.1109/fose.2007.16},
 month = {may},
 publisher = {{IEEE}},
 title = {Research Collaborations between Academia and Industry},
 url = {https://doi.org/10.1109%2Ffose.2007.16},
 year = {2007}
}

@article{Teece_1977,
 abstract = {The following sections are included:IntroductionTechnology Transfer and the Production of KnowledgeThe SampleDefinition of Technology Transfer CostsTransfer Costs: Data and HypothesesThe Level of Transfer CostsTechnology/Transferor CharacteristicsTransferee and Host Country CharacteristicsDeterminants of the Cost of International Technology Transfer: Tests and ResultsThe ModelStatistical Tests: Phase IStatistical Tests: Phase IIDefferences between International and Domestic Technology TransferConclusion
References},
 author = {D. J. Teece},
 doi = {10.2307/2232084},
 journal = {The Economic Journal},
 month = {jun},
 number = {346},
 pages = {242},
 publisher = {Oxford University Press ({OUP})},
 title = {Technology Transfer by Multinational Firms: The Resource Cost of Transferring Technological Know-How},
 url = {https://doi.org/10.2307%2F2232084},
 volume = {87},
 year = {1977}
}

@article{Zelkowitz_1996,
 abstract = {Abstract technology transfer is of crucial concern to both
government and industry today. In this paper, several software
engineering technologies used within NASA are studied, and the
mechanisms, schedules, and efforts at transferring these technologies
are investigated. The goals of this study are: (1) to understand the
difference between technology transfer (the adoption of a new method by
large segments of an industry) as an industrywide phenomenon and the
adoption of a new technology by an individual organization (called
technology infusion); and (2) to see if software engineering technology
transfer differs from other engineering disciplines. While there is
great interest today in developing technology transfer models for
industry, it is the technology infusion process that actually causes
changes in the current state of the practice},
 author = {M.V. Zelkowitz},
 doi = {10.1109/17.511836},
 journal = {{IEEE} Transactions on Engineering Management},
 number = {3},
 pages = {250--261},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Software engineering technology infusion within {NASA}},
 url = {https://doi.org/10.1109%2F17.511836},
 volume = {43},
 year = {1996}
}

@inproceedings{Colyer_2000,
 abstract = {Over a five year period the Applied Science & Technology group of IBM's Hursley Laboratory in England turned itself from a fully-funded research organisation into an entirely self-funded technology transfer group. Much practical experience and insight was gained into the questions of: What are the obstacles to overcome in successful technology transfer? How to find a match between technology and customer? How best to manage risk and expectation?To be successful a technology transfer group needs to be correctly positioned within its sponsoring organisation, use management processes that provide flexibility and control, and develop a sophisticated engagement model for working with its customers.},
 author = {Adrian M. Colyer},
 booktitle = {Proceedings of the 22nd international conference on Software engineering  - {ICSE} {\textquotesingle}00},
 doi = {10.1145/337180.337467},
 publisher = {{ACM} Press},
 title = {From research to reward},
 url = {https://doi.org/10.1145%2F337180.337467},
 year = {2000}
}

@article{Gorschek_2006,
 abstract = {Technology transfer, and thus industry-relevant research, involves more than merely producing research results and delivering them in publications and technical reports. It demands close cooperation and collaboration between industry and academia throughout the entire research process. During research conducted in a partnership between Blekinge Institute of Technology and two companies, Danaher Motion Saro AB (DHR) and ABB, we devised a technology transfer model that embodies this philosophy. We initiated this partnership to conduct industry-relevant research in requirements engineering and product management. Technology transfer in this context is a prerequisite: it validates academic research results in a real setting, and it provides a way to improve industry development and business processes},
 author = {Tony Gorschek and Per Garre and Stig Larsson and Claes Wohlin},
 doi = {10.1109/ms.2006.147},
 journal = {{IEEE} Software},
 month = {nov},
 number = {6},
 pages = {88--95},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {A Model for Technology Transfer in Practice},
 url = {https://doi.org/10.1109%2Fms.2006.147},
 volume = {23},
 year = {2006}
}

@article{Hall_2002,
 abstract = {A study of the problems experienced by twelve software companies in their requirements process is discussed. The aim of the work is to develop a more holistic understanding of the requirements process, so that companies can more effectively organise and manage requirements. The findings suggest that most requirements problems are organisational rather than technical, and that there is a relationship between companies' maturity and patterns of requirements problems.},
 author = {T. Hall and S. Beecham and A. Rainer},
 doi = {10.1049/ip-sen:20020694},
 journal = {{IEE} Proceedings - Software},
 number = {5},
 pages = {153},
 publisher = {Institution of Engineering and Technology ({IET})},
 title = {Requirements problems in twelve software companies: an empirical analysis},
 url = {https://doi.org/10.1049%2Fip-sen%3A20020694},
 volume = {149},
 year = {2002}
}

@inproceedings{Hietala,
 abstract = {We report findings from a survey that has been conducted in Finland to study how the software product companies have matured and evolved over the years. In addition to introducing some key terms for characterizing the software product business, we provide some overall data on the sector, and discuss some specific issues related to the software R&D process and subcontracting. The survey is one of the largest such surveys covering the software product companies and has revealed several interesting findings on software product companies. Most notably, the software companies have shown to be dynamic and resilient in challenging business environment; and their biggest challenges in growth are not technical but management and marketing related. Furthermore, we also discovered that the most important improvement areas are improving the degree of productization and level of competence of personnel and that the ability to network with other companies is critical for younger companies. This survey also revealed that programming and planning are the two most common types of subcontracting, and difficulties in modularity and specifications are the biggest hurdles that prevent wider use of subcontracting.},
 author = {J. Hietala and J. Kontio and J.-P. Jokinen and J. Pyysiainen},
 booktitle = {10th International Symposium on Software Metrics, 2004. Proceedings.},
 doi = {10.1109/metric.2004.1357906},
 publisher = {{IEEE}},
 title = {Challenges of software product companies: results of a national survey in finland},
 url = {https://doi.org/10.1109%2Fmetric.2004.1357906}
}

@article{Karlsson_2007,
 abstract = {Requirements engineering for market-driven software development entails special challenges. This paper presents results from an empirical study that investigates these challenges, taking a qualitative approach using interviews with fourteen employees at eight software companies and a focus group meeting with practitioners. The objective of the study is to increase the understanding of the area of market-driven requirements engineering and provide suggestions for future research by describing encountered challenges. A number of challenging issues were found, including bridging communication gaps between marketing and development, selecting the right level of process support, basing the release plan on uncertain estimates, and managing the constant flow of requirements.},
 author = {Lena Karlsson and {\AA}sa G. Dahlstedt and Björn Regnell and Johan Natt och Dag and Anne Persson},
 doi = {10.1016/j.infsof.2007.02.008},
 journal = {Information and Software Technology},
 month = {jun},
 number = {6},
 pages = {588--604},
 publisher = {Elsevier {BV}},
 title = {Requirements engineering challenges in market-driven software development {\textendash} An interview study with practitioners},
 url = {https://doi.org/10.1016%2Fj.infsof.2007.02.008},
 volume = {49},
 year = {2007}
}

@article{Morris_1998,
 abstract = {In September 1996 the Joint Research Centre of the European Commission held a workshop in Brussels on problems with industrial uptake from research and development projects in Requirements Engineering. Although there have, in this domain, been a number of research projects industrial uptake has rarely lived up to expectations. The workshop set out to investigate possible explanations for this and what potential mechanisms there may be for promoting industrial uptake of current and future Requirement Engineering projects. This paper has two functions: to describe the results of this workshops and to provide a framework to support planning for future RE activities and initiatives.},
 author = {Philip Morris and Marcelo Masera and Marc Wilikens},
 doi = {10.1007/bf02919966},
 journal = {Requirements Engineering},
 month = {jun},
 number = {2},
 pages = {79--83},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Requirements engineering and industrial uptake},
 url = {https://doi.org/10.1007%2Fbf02919966},
 volume = {3},
 year = {1998}
}

@article{Pinheiro_2003,
 abstract = {This paper is an experience report on an initiative to incorporate requirements engineering practices into the development process of two large corporations. The project was framed by a government program to foster collaboration between academia, industry, and government agencies. Its expected result—proposals for cooperative work—is in itself a major challenge, since it was not decided from the beginning what aspect of requirements engineering would be covered by the proposals.We describe the cooperation process followed by the project partners, three universities, and two companies, the difficulties found therein, and compare our experience with other reports on the issue of adoption of requirements engineering into the mainstream practice.},
 author = {Francisco A. C. Pinheiro and Julio Cesar Sampaio do Prado Leite and Jaelson F. B. Castro},
 doi = {10.1023/a:1022994516210},
 journal = {The Journal of Technology Transfer},
 number = {2},
 pages = {159--165},
 publisher = {Springer Science and Business Media {LLC}},
 url = {https://doi.org/10.1023%2Fa%3A1022994516210},
 volume = {28},
 year = {2003}
}

@article{Sawyer_2001,
 abstract = {This article discusses how mathematics has explicated and evaluated computational possibilities by sketching exact boundaries for the world of computers. This article also explains how mathematics has extended these boundaries for computation immensely. What is impossible from the point of view of traditional mathematics and has been argued as absolutely unworkable becomes attainable in light of new mathematics. The other aspiration is to show how to utilize new possibilities. To achieve this, a new paradigm for computation is introduced. It gives better insight into the functioning of computers, networks and the mind; opening new perspectives for artificial intelligence. Mathematics is a powerful tool that helps people achieve new goals as well as understand what is impossible to do. Today, the theory of algorithms helps solve such vital practical problems as Web reliability, communication security, computer efficiency, and many others. The algorithm has become one of the central concepts of mathematics and a general scientific and technological notion. The theory of super-recursive computing has an important message for society as well as technology. It only looks smart to stop and to enjoy after one get something.},
 author = {Steve Sawyer},
 doi = {10.1145/384150.384168},
 journal = {Communications of the {ACM}},
 month = {nov},
 number = {11},
 pages = {97--102},
 publisher = {Association for Computing Machinery ({ACM})},
 title = {A market-based perspective on information systems development},
 url = {https://doi.org/10.1145%2F384150.384168},
 volume = {44},
 year = {2001}
}

@article{Barnes_2002,
 abstract = {There is a growing world-wide trend toward greater collaboration between academia and industry, an activity encouraged by governments as a means of enhancing national competitiveness and wealth creation. Warwick Manufacturing Group (WMG) is well known for its extensive links with industry, and provided an excellent opportunity for a study of management practice within university–industry collaborative research projects. This paper evaluates the findings of six collaborative research projects. The objective was to identify factors which, if managed correctly, increase the probability of a collaboration being perceived as successful by both academic and industrial partners. The outcome was a good practice model for successful university–industry research collaborations.},
 author = {Tina Barnes and Ian Pashby and Anne Gibbons},
 doi = {10.1016/s0263-2373(02)00044-0},
 journal = {European Management Journal},
 month = {jun},
 number = {3},
 pages = {272--285},
 publisher = {Elsevier {BV}},
 title = {Effective University {\textendash} Industry Interaction:},
 url = {https://doi.org/10.1016%2Fs0263-2373%2802%2900044-0},
 volume = {20},
 year = {2002}
}

@article{Gorschek_2006,
 abstract = {Technology transfer, and thus industry-relevant research, involves more than merely producing research results and delivering them in publications and technical reports. It demands close cooperation and collaboration between industry and academia throughout the entire research process. During research conducted in a partnership between Blekinge Institute of Technology and two companies, Danaher Motion Saro AB (DHR) and ABB, we devised a technology transfer model that embodies this philosophy. We initiated this partnership to conduct industry-relevant research in requirements engineering and product management. Technology transfer in this context is a prerequisite: it validates academic research results in a real setting, and it provides a way to improve industry development and business processes},
 author = {Tony Gorschek and Per Garre and Stig Larsson and Claes Wohlin},
 doi = {10.1109/ms.2006.147},
 journal = {{IEEE} Software},
 month = {nov},
 number = {6},
 pages = {88--95},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {A Model for Technology Transfer in Practice},
 url = {https://doi.org/10.1109%2Fms.2006.147},
 volume = {23},
 year = {2006}
}

@article{Mathiassen_2002,
 abstract = {Reports from a systems development research tradition in which emphasis is put on relating research activities to practice and on establishing fruitful collaboration between groups of researchers and practitioners. Describes and evaluates a specific research project in which a large group of researchers and practitioners worked together to understand, support, and improve systems development practices in four organisations over a period of three years. Uses the case to reflect on the research goals, approaches, and results involved in this tradition for researching systems development practice. Proposes collaborative practice research as a way to organise and conduct research into systems development practice based on close collaboration between researchers and practitioners. Exemplifies the use of pluralist research methodology by combining action research with experiments and conventional practice studies. Argues that collaborative practice research offers one practical way to strike a useful balance between relevance and rigour. Concludes with a discussion of the implications for the relation between research and practice within the systems development discipline and with lessons on how to design research efforts as collaborations between researchers and practitioners.},
 author = {Lars Mathiassen},
 doi = {10.1108/09593840210453115},
 journal = {Information Technology {\&}amp$\mathsemicolon$ People},
 month = {dec},
 number = {4},
 pages = {321--345},
 publisher = {Emerald},
 title = {Collaborative practice research},
 url = {https://doi.org/10.1108%2F09593840210453115},
 volume = {15},
 year = {2002}
}

@article{Mora_Valentin_2004,
 abstract = {This paper reviews the economic literature on the role of fees in patent systems. Two main research questions are usually addressed: the impact of patent fees on the behavior of applicants and the question of optimal fees. Studies in the former group confirm that a range of fees affect the behavior of applicants and suggest that a patent is an inelastic good. Studies in the latter group provide grounds for both low and high application (or pre-grant) fees and renewal (or post-grant) fees, depending on the structural context and on the policy objectives. The paper also presents new stylized facts on patent fees of thirty patent offices worldwide. It is shown that application fees are generally lower than renewal fees, and renewal fees increase more than proportionally with patent age (to the notable exception of Switzerland and the U.S.).},
 author = {Eva M Mora-Valentin and Angeles Montoro-Sanchez and Luis A Guerras-Martin},
 doi = {10.1016/s0048-7333(03)00087-8},
 journal = {Research Policy},
 month = {jan},
 number = {1},
 pages = {17--40},
 publisher = {Elsevier {BV}},
 title = {Determining factors in the success of R{\&}amp$\mathsemicolon$D cooperative agreements between firms and research organizations},
 url = {https://doi.org/10.1016%2Fs0048-7333%2803%2900087-8},
 volume = {33},
 year = {2004}
}

@inproceedings{Rombach_2007,
 abstract = {The rapid and complex research and innovation processes require high-tech companies to optimize their technology transfer processes. It is clearly not sufficient to solely rely on internal R&D; strategic cooperations with external research centers of excellence are needed in order to compete in the global innovation market. Candidates for such strategic cooperations are universities, research institutions, and technology focused consulting companies. Key challenge is the effective integration of external competences into the company-internal innovation processes. In this paper we present a survey of the state-of-the-art in technology transfer, high-light promising success cases for the future, and derive success criteria for successful technology transfer in a global world. The cooperation between Siemens and Fraunhofer IESE is presented as a concrete example.},
 author = {Dieter Rombach and Reinhold Achatz},
 booktitle = {Future of Software Engineering ({FOSE} {\textquotesingle}07)},
 doi = {10.1109/fose.2007.16},
 month = {may},
 publisher = {{IEEE}},
 title = {Research Collaborations between Academia and Industry},
 url = {https://doi.org/10.1109%2Ffose.2007.16},
 year = {2007}
}

@article{Stelzer_1998,
 abstract = {},
 author = {Dirk Stelzer and Werner Mellis},
 doi = {10.1002/(sici)1099-1670(199812)4:4<227::aid-spip106>3.0.co;2-1},
 journal = {Software Process: Improvement and Practice},
 month = {dec},
 number = {4},
 pages = {227--250},
 publisher = {Wiley},
 title = {Success factors of organizational change in software process improvement},
 url = {https://doi.org/10.1002%2F%28sici%291099-1670%28199812%294%3A4%3C227%3A%3Aaid-spip106%3E3.0.co%3B2-1},
 volume = {4},
 year = {1998}
}

@article{2000,
 abstract = {},
 doi = {10.1016/s0898-1221(00)90203-7},
 journal = {Computers {\&}amp$\mathsemicolon$ Mathematics with Applications},
 month = {jul},
 number = {2-3},
 pages = {418},
 publisher = {Elsevier {BV}},
 title = {Experimentation in software engineering: An introduction},
 url = {https://doi.org/10.1016%2Fs0898-1221%2800%2990203-7},
 volume = {40},
 year = {2000}
}

@book{2000,
 abstract = {Software product lines are emerging as a critical new paradigm for software development. Product lines are enabling organizations to achieve impressive time-to-market gains and cost reductions. With the increasing number of product lines and product-line researchers and practitioners, the time is right for a comprehensive examination of the issues surrounding the software product line approach. The Software Engineering Institute at Carnegie Mellon University is proud to sponsor the first conference on this important subject.
This book comprises the proceedings of the First Software Product Line Conference (SPLC1), held August 28-31, 2000, in Denver, Colorado, USA. The twenty-seven papers of the conference technical program present research results and experience reports that cover all aspects of software product lines. Topics include business issues, enabling technologies, organizational issues, and life-cycle issues. Emphasis is placed on experiences in the development and fielding of product lines of complex systems, especially those that expose problems in the design, development, or evolution of software product lines. The book will be essential reading for researchers and practitioners alike.},
 doi = {10.1007/978-1-4615-4339-8},
 editor = {Patrick Donohoe},
 publisher = {Springer {US}},
 title = {Software Product Lines},
 url = {https://doi.org/10.1007%2F978-1-4615-4339-8},
 year = {2000}
}

@article{Bril_2000,
 abstract = {},
 author = {Reinder J. Bril and Loe M. G. Feijs and Andr� Glas and Ren� L. Krikhaar and M. (Thijs) R. M. Winter},
 doi = {10.1002/1096-908x(200005/06)12:3<143::aid-smr207>3.0.co;2-7},
 journal = {Journal of Software Maintenance: Research and Practice},
 number = {3},
 pages = {143--170},
 publisher = {Wiley},
 title = {Maintaining a legacy: towards support at the architectural level},
 url = {https://doi.org/10.1002%2F1096-908x%28200005%2F06%2912%3A3%3C143%3A%3Aaid-smr207%3E3.0.co%3B2-7},
 volume = {12},
 year = {2000}
}

@article{Bril_2005,
 abstract = {Software architecture plays a vital role in the development (and hence maintenance) of large complex systems (containing millions of lines of code) with a long lifetime. It is therefore required that the software architecture is also maintained, i.e., sufficiently documented, clearly communicated, and explicitly controlled during its life-cycle. In our experience, these requirements cannot be met without appropriate support.
Commercial-off-the-shelf support for architectural maintenance is still scarcely available, if at all, implying the need to develop appropriate proprietary means. In this paper, we reflect upon software architecture maintenance taken within three organizations within Philips that develop professional systems. We extensively describe the experience gained with introducing and embedding of architectural support in these three organizations. We focus on architectural support in the area of software architecture recovery, visualization, analysis, and verification.
In our experience, the support must be carried by a number of pillars of software development, and all of these pillars have to go through a change process to ensure sustainable embedding. Managing these changes requires several key roles to be fulfilled in the organization: a champion, a company angel, a change agent, and a target. We call our reflection model C-POSH, which is an acronym for Change management of the four identified pillars of software development: Process, Organization, Software development environment, and Humans. Our experiences will be presented in terms of the C-POSH model. Copyright},
 author = {R. J. Bril and R. L. Krikhaar and A. Postma},
 doi = {10.1002/smr.304},
 journal = {Journal of Software Maintenance and Evolution: Research and Practice},
 month = {jan},
 number = {1},
 pages = {3--25},
 publisher = {Wiley},
 title = {Architectural support in industry: a reflection using C-{POSH}},
 url = {https://doi.org/10.1002%2Fsmr.304},
 volume = {17},
 year = {2005}
}

@article{Buxton_1991,
 abstract = {The paper puts forward a possible generic model for the process of
technology transfer. It describes in a generalised style how the process
takes place in typical industrial circumstances, ranging from initial
awareness to in-use support. One might image that the example under
study is the adoption of a new software engineering method and
associated toolset in a company, in a business which makes substantial
use of computers embedded in its products. This model relates primarily
to the transfer of discrete software process technology, i.e. a new
method or toolset. The model is based on experience, gained over the
last 15 years},
 author = {J.N. Buxton and R. Malcolm},
 doi = {10.1049/sej.1991.0002},
 journal = {Software Engineering Journal},
 number = {1},
 pages = {17},
 publisher = {Institution of Engineering and Technology ({IET})},
 title = {Software technology transfer},
 url = {https://doi.org/10.1049%2Fsej.1991.0002},
 volume = {6},
 year = {1991}
}

@article{Feijs_1998,
 abstract = {This paper reports on our experience with a relational approach to support the analysis of existing software architectures. The analysis options provide for visualization and view calculation. The approach has been applied for reverse engineering. It is also possible to check concrete designs against architecture-related rules. The paper surveys the theory, the tools and some of the applications developed so far. © 1998 John Wiley & Sons, Ltd.},
 author = {L. Feijs and R. Krikhaar and R. Van Ommering},
 doi = {10.1002/(sici)1097-024x(19980410)28:4<371::aid-spe154>3.0.co;2-1},
 journal = {Software: Practice and Experience},
 month = {apr},
 number = {4},
 pages = {371--400},
 publisher = {Wiley},
 title = {A relational approach to support software architecture analysis},
 url = {https://doi.org/10.1002%2F%28sici%291097-024x%2819980410%2928%3A4%3C371%3A%3Aaid-spe154%3E3.0.co%3B2-1},
 volume = {28},
 year = {1998}
}

@article{Jaring_2003,
 abstract = {Promoting software reuse is probably the most promising approach to the cost-effective development and evolution of quality software. An example of reuse is the successful adoption of software product families in industry. In a product family context, software architects anticipate product variation and design architectures that support product derivation in both space (multiple contexts) and time (changing contexts). Product derivation is based on the concept of variability: a single architecture and a set of components support a family of products. Modern software product families need to support increasing amounts of variability, leading to a situation where variability engineering becomes of primary concern. Variability is often introduced as an "add-on" to the system without taking the consequences for more than one lifecycle phase such as design or architecture into account. This paper suggests (1) a Variability Categorization and Classification Model (VCCM) for representing variability in the software lifecycle and (2) discusses a case study of a large-scale software product family of MRI scanners developed by Philips Medical Systems. The study illustrates how variability can be made an integral part of system development at different levels of abstraction. VCCM has been applied to the scanner family as an analysis tool.},
 author = {M. Jaring and R. L. Krikhaar and J. Bosch},
 doi = {10.1002/spe.558},
 journal = {Software: Practice and Experience},
 month = {dec},
 number = {1},
 pages = {69--100},
 publisher = {Wiley},
 title = {Representing variability in a family of {MRI} scanners},
 url = {https://doi.org/10.1002%2Fspe.558},
 volume = {34},
 year = {2003}
}

@article{Kimberly_1988,
 abstract = {},
 author = {John R. Kimberly and Walter R. Nord and Sharon Tucker},
 doi = {10.2307/2393064},
 journal = {Administrative Science Quarterly},
 month = {jun},
 number = {2},
 pages = {314},
 publisher = {{JSTOR}},
 title = {Implementating Routine and Radical Innovations.},
 url = {https://doi.org/10.2307%2F2393064},
 volume = {33},
 year = {1988}
}

@inproceedings{Krasner,
 abstract = {From observations gathered during a study of large software development projects, it was concluded that the software engineering technology (SET) transfer process is plagued with problems involving learning, technical communication, and negotiation. The underlying causes of these problems are discussed. The observations presented here attempted to help facilitate the MCC Software Technology program technology transfer of advanced software development technology, which mostly failed. Technology transfer was a major goal and risk of the MCC Software Technology program.},
 author = {H. Krasner},
 booktitle = {Proceedings of the Twenty-Eighth Hawaii International Conference on System Sciences},
 doi = {10.1109/hicss.1995.375680},
 publisher = {{IEEE} Comput. Soc. Press},
 title = {Bottlenecks in the transfer of software engineering technology: lessons learned from a consortium failure},
 url = {https://doi.org/10.1109%2Fhicss.1995.375680}
}

@article{Krikhaar_2007,
 abstract = {Configuration management tools have become well and widely accepted by the software industry. Software Configuration Management (SCM) systems hold minute information about the entire evolution of complex software systems and thus represent a good source ...},
 author = {Ren{\'{e}} Krikhaar and Ivica Crnkovic},
 doi = {10.1016/j.scico.2006.10.003},
 journal = {Science of Computer Programming},
 month = {apr},
 number = {3},
 pages = {215--221},
 publisher = {Elsevier {BV}},
 title = {Software Configuration Management},
 url = {https://doi.org/10.1016%2Fj.scico.2006.10.003},
 volume = {65},
 year = {2007}
}

@article{Kruchten_1995,
 abstract = {The 4 + 1 View Model describes software architecture using five concurrent views, each of which addresses a specific set of concerns: The logical view describes the design's object model, the process view describes the design's concurrency and synchronization aspects; the physical view describes the mapping of the software onto the hardware and shows the system's distributed aspects, and the development view describes the software's static organization in the development environment. Software designers can organize the description of their architectural decisions around these four views and then illustrate them with a few selected use cases, or scenarios, which constitute a fifth view. The architecture is partially evolved from these scenarios.The 4+1 View Model allows various stakeholders to find what they need in the software architecture. System engineers can approach it first from the physical view, then the process view; end users, customers, and data specialists can approach it from the logical view; and project managers and software-configuration staff members can approach it from the development view.},
 author = {P.B. Kruchten},
 doi = {10.1109/52.469759},
 journal = {{IEEE} Software},
 number = {6},
 pages = {42--50},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {The 4$\mathplus$1 View Model of architecture},
 url = {https://doi.org/10.1109%2F52.469759},
 volume = {12},
 year = {1995}
}

@inproceedings{Larsson_2006,
 abstract = {In this paper we present several technology transfer cases both successes and failures. These cases describe transfers from research department within a large corporation to a product company, university to a large product company and university to a small product company. Based on the analysis of these cases we outline several strategies that can be used, in isolation or combined, in order to increase the probability for success.},
 author = {Magnus Larsson and Anders Wall and Christer Norström and Ivica Crnkovic},
 booktitle = {Proceedings of the 2006 international workshop on Software technology transfer in software engineering},
 doi = {10.1145/1138046.1138055},
 month = {may},
 publisher = {{ACM}},
 title = {Technology transfer},
 url = {https://doi.org/10.1145%2F1138046.1138055},
 year = {2006}
}

@incollection{Leinonen_2002,
 abstract = {Normally, it is challenging to find funding and to get the best R& D engineers to participate in quality and process improvement 
programs. The payback time is considerably long, and the benefits difficult to express reliably in monetary terms. How can
you then proceed under tight business conditions: stable or declining markets with deteriorating margins?

The target of the presentation is to go through a few commonly recognized SW engineering paradigms such as the CMMI, requirements
engineering, productized engineering environments, and the SW architecture process and to elaborate how we have been able
to or plan to apply them in the current business environment.},
 author = {Timo-Pekka Leinonen},
 booktitle = {Product Focused Software Process Improvement},
 doi = {10.1007/3-540-36209-6_2},
 pages = {2--2},
 publisher = {Springer Berlin Heidelberg},
 title = {Keynote Address: {SW} Engineering under Tight Economic Constrains},
 url = {https://doi.org/10.1007%2F3-540-36209-6_2},
 year = {2002}
}

@article{Pentinmaki_2004,
 abstract = {This report is intended to summarize the proceedings of the Second International Workshop on Detection of Software Clones (IWDSC'2003). The aim of the workshop was to bring together researchers within the field of clone detection to critically assess ...},
 author = {Isaac Pentinmaki},
 doi = {10.1145/979743.979773},
 journal = {{ACM} {SIGSOFT} Software Engineering Notes},
 month = {mar},
 number = {2},
 pages = {33--33},
 publisher = {Association for Computing Machinery ({ACM})},
 title = {Review of "Lean software development},
 url = {https://doi.org/10.1145%2F979743.979773},
 volume = {29},
 year = {2004}
}

@article{Pfleeger_1999,
 abstract = {Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.},
 author = {S.L Pfleeger},
 doi = {10.1016/s0164-1212(99)00031-x},
 journal = {Journal of Systems and Software},
 month = {jul},
 number = {2-3},
 pages = {111--124},
 publisher = {Elsevier {BV}},
 title = {Understanding and improving technology transfer in software engineering},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900031-x},
 volume = {47},
 year = {1999}
}

@article{Pfleeger_2000,
 abstract = {We can learn much from the business community about effective
technology transfer. In particular, understanding the interests of
different types of adopters can suggest to us the different kinds of
evidence needed to convince someone to try an innovative technology. At
the same time, the legal community offers us advice about what kinds of
evidence are needed to build convincing cases that an innovation is an
improvement over current practice. The article examines why and how we
make technology selection decisions and also considers how evidence
supporting these decisions helps or hinders the adoption of new
technology},
 author = {S.L. Pfleeger and W. Menezes},
 doi = {10.1109/52.819965},
 journal = {{IEEE} Software},
 number = {1},
 pages = {27--33},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Marketing technology to software practitioners},
 url = {https://doi.org/10.1109%2F52.819965},
 volume = {17},
 year = {2000}
}

@inproceedings{Pietersma,
 abstract = {Fault diagnosis is crucial for the reduction of test & integration time and down-time of complex systems. In this paper, we present a model-based approach to derive tests and test sequences for sequential fault diagnosis. This approach offers advantages over methods that are based on test coverage of explicit fault states, represented in matrix form. Functional models are more easily adapted to design changes and constitute a complete information source for test selection on a given abstraction level. We introduce our approach and implementation with a theoretic example. We demonstrate the practical use in three case studies and obtain cost reductions of up to 59% compared to the matrix-based approach},
 author = {J. Pietersma and A.J.C. van Gemund and A. Bos},
 booktitle = {{IEEE} Autotestcon, 2005.},
 doi = {10.1109/autest.2005.1609208},
 publisher = {{IEEE}},
 title = {A model-based approach to sequential fault diagnosis},
 url = {https://doi.org/10.1109%2Fautest.2005.1609208}
}

@article{Pietersma_2007,
 abstract = {Fault diagnosis is crucial for the reduction of test and integration time as well as downtime of complex systems. In this article, we present a model-based approach to derive tests and test sequences for sequential fault diagnosis. This approach offers advantages over methods that are based on test coverage of explicit fault states, represented in matrix form. Functional models are more easily adapted to design changes and constitute a complete information source for test selection on a given abstraction level. We introduce our approach and implementation with a theoretical example. We demonstrate its practical use in three case studies, and for these cases we obtain cost reductions of up to 59% compared to the matrix-based approach},
 author = {Jurryt Pietersma and Arjan J.c. Van Gemund and Andre Bos},
 doi = {10.1109/mim.2007.364961},
 journal = {{IEEE} Instrumentation {\&}amp$\mathsemicolon$ Measurement Magazine},
 month = {apr},
 number = {2},
 pages = {46--52},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {A model-based approach to sequential fault diagnosis - A best student paper award winner at {IEEE} Autotestcon 2005},
 url = {https://doi.org/10.1109%2Fmim.2007.364961},
 volume = {10},
 year = {2007}
}

@article{Postma_2003,
 abstract = {A method for module architecture verification is described, which yields support for checking on an architectural level whether the implicit module architecture of the implementation of a system is consistent with its specified module architecture, and which facilitates achieving architecture conformance by relating architectural-level violations to the code-level entities that cause them, hence making it easier to resolve them. Module architecture conformance is needed to enable implementing and maintaining the system and reasoning about it. We describe our experience having applied the proposed method to check a representative part of the module architecture of a large industrial component-based software system.},
 author = {Andr{\'{e}} Postma},
 doi = {10.1016/s0950-5849(02)00193-3},
 journal = {Information and Software Technology},
 month = {mar},
 number = {4},
 pages = {171--194},
 publisher = {Elsevier {BV}},
 title = {A method for module architecture verification and its application on a large component-based system},
 url = {https://doi.org/10.1016%2Fs0950-5849%2802%2900193-3},
 volume = {45},
 year = {2003}
}

@article{Potts_1993,
 abstract = {The author discusses three major changes that he suggests are occurring as a result of the software engineering industry adopting the industry-as-laboratory approach, in which researchers identify problems through close involvement with industrial projects and create and evaluate solutions in an almost indivisible research activity. This approach emphasizes what people actually do or can do in practice, rather than what is possible in principle. The three changes are a greater reliance on empirical definition of problems, an emphasis on real case studies, and a greater emphasis on contextual issues.< >},
 author = {C. Potts},
 doi = {10.1109/52.232392},
 journal = {{IEEE} Software},
 month = {sep},
 number = {5},
 pages = {19--28},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Software-engineering research revisited},
 url = {https://doi.org/10.1109%2F52.232392},
 volume = {10},
 year = {1993}
}

@incollection{Pronk_2000,
 abstract = {In the medical imaging domain the market pressure and increased software content of products form the driving force for the introduction of product line architectures. This paper describes a new product line architecture that is based upon a generic platform that can only be varied through well-defined interfaces. It explains the rationale behind this approach and elaborates on the experiences so far. The paper gives a short overview of the architectural requirements for the product line and the methodology followed. The focus is on the actual platform architecture and its technical implementation.},
 author = {B. J. Pronk},
 booktitle = {Software Product Lines},
 doi = {10.1007/978-1-4615-4339-8_18},
 pages = {331--351},
 publisher = {Springer {US}},
 title = {An Interface-Based Platform Approach},
 url = {https://doi.org/10.1007%2F978-1-4615-4339-8_18},
 year = {2000}
}

@inproceedings{Soni_1995,
 abstract = {To help us identify and focus on pragmatic and concrete issues related to the role of software architecture in the design and development of large systems, we conducted a survey of a variety of software systems used in industrial applications. Our premise, which guided the examination of these systems, was that software architecture is concerned with capturing the structures of a system and the relationships among the elements both within and between structures. The structures we found fell into several broad categories: conceptual structure, module structure, code structure, and execution structure. These categories address different engineering concerns. The separation of such concerns, combined with specialized implementation techniques, decreased the complexity of implementation, and improved reuse and reconfiguration. We observed that in practice, software architecture played an important role throughout the development process: specification, design, functional decompo...},
 author = {Dilip Soni and Robert L. Nord and Christine Hofmeister},
 booktitle = {Proceedings of the 17th international conference on Software engineering  - {ICSE} {\textquotesingle}95},
 doi = {10.1145/225014.225033},
 publisher = {{ACM} Press},
 title = {Software architecture in industrial applications},
 url = {https://doi.org/10.1145%2F225014.225033},
 year = {1995}
}

@inproceedings{van_Deursen_1999,
 abstract = {},
 author = {Arie van Deursen and Tobias Kuipers},
 booktitle = {Proceedings of the 21st international conference on Software engineering},
 doi = {10.1145/302405.302629},
 month = {may},
 publisher = {{ACM}},
 title = {Identifying objects using cluster and concept analysis},
 url = {https://doi.org/10.1145%2F302405.302629},
 year = {1999}
}

@article{van_Ommering_2000,
 abstract = {Most consumer electronics today contain embedded software. In the
early days, developing CE software presented relatively minor
challenges, but in the past several years three significant problems
have arisen: size and complexity of the software in individual products;
the increasing diversity of products and their software; and the need
for decreased development time. The question of handling diversity and
complexity in embedded software at an increasing production speed
becomes an urgent one. The authors present their belief that the answer
lies not in hiring more software engineers. They are not readily
available, and even if they were, experience shows that larger projects
induce larger lead times and often result in greater complexity.
Instead, they believe that the answer lies in the use and reuse of
software components that work within an explicit software architecture.
The Koala model, a component-oriented approach detailed in this article,
is their way of handling the diversity of software in consumer
electronics. Used for embedded software in TV sets, it allows late
binding of reusable components with no additional overhead},
 author = {R. van Ommering and F. van der Linden and J. Kramer and J. Magee},
 doi = {10.1109/2.825699},
 journal = {Computer},
 month = {mar},
 number = {3},
 pages = {78--85},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {The Koala component model for consumer electronics software},
 url = {https://doi.org/10.1109%2F2.825699},
 volume = {33},
 year = {2000}
}

@incollection{Aschauer_2008,
 abstract = {This paper sketches a recent successful requirements analysis of a complex industrial automation system that mainly required a talented expert, with a beginner's mind, who has been willing to dig into the domain details together with a committed customer and a motivated team. With these key factors and the application of an appropriate combination of well-established and some newer methods and tools, we were able to efficiently elicit, refine, and validate requirements. From this specific context, we try to derive implications for innovative requirements analysis. We argue that in projects that go beyond simple, well defined, and well understood applications, automated requirements analysis is unlikely to lead to a successful specification of a system.},
 author = {Thomas Aschauer and Gerd Dauenhauer and Patricia Derler and Wolfgang Pree and Christoph Steindl},
 booktitle = {Lecture Notes in Computer Science},
 doi = {10.1007/978-3-540-89778-1_6},
 pages = {25--42},
 publisher = {Springer Berlin Heidelberg},
 title = {Could an Agile Requirements Analysis Be Automated?{\textemdash}Lessons Learned from the Successful Overhauling of an Industrial Automation System},
 url = {https://doi.org/10.1007%2F978-3-540-89778-1_6},
 year = {2008}
}

@inproceedings{Aschauer_2009,
 abstract = {This paper presents the architecture of a model-driven engineering framework which relies on the unified notion of classes and objects, as pioneered by SELF. We implemented this architecture for the domain of testbed automation systems and argue that this architecture can be generalized. We outline why our first prototype implementation following a conventional, UML-like metamodeling approach failed and how the follow-up implementation is aligned with the more appropriate so-called orthogonal classification architecture (OCA). While the OCA has been thoroughly studied theoretically, we applied OCA for a real-world case in the automation system domain. We demonstrate that this modeling approach is feasible and implies a straightforward, clear-cut decomposition of the framework into implementation modules, leading to comprehensible software architectures.},
 author = {Thomas Aschauer and Gerd Dauenhauer and Wolfgang Pree},
 booktitle = {2009 Joint Working {IEEE}/{IFIP} Conference on Software Architecture {\&}amp$\mathsemicolon$ European Conference on Software Architecture},
 doi = {10.1109/wicsa.2009.5290798},
 month = {sep},
 publisher = {{IEEE}},
 title = {Towards a generic architecture for multi-level modeling},
 url = {https://doi.org/10.1109%2Fwicsa.2009.5290798},
 year = {2009}
}

@incollection{Aschauer_2009,
 abstract = {Multi-level modeling using so-called clabjects has been proposed as an alternative to UML for modeling domains that feature more than one classi- fication level. In real-world applications, however , this modeling formalism has not yet become popular, because it is a challenge t o efficiently represent large models, and providing fast access to all informatio n spread across the meta- levels at the same time. In this paper we present t he model representation con- cept that relies on a permanent condensed view of t he model, the corresponding traversal algorithms, and their implementations tha t proved adequate for model- driven engineering of industrial automation systems consisting of hundreds of thousands of model elements.},
 author = {Thomas Aschauer and Gerd Dauenhauer and Wolfgang Pree},
 booktitle = {Model Driven Engineering Languages and Systems},
 doi = {10.1007/978-3-642-04425-0_3},
 pages = {17--31},
 publisher = {Springer Berlin Heidelberg},
 title = {Representation and Traversal of Large Clabject Models},
 url = {https://doi.org/10.1007%2F978-3-642-04425-0_3},
 year = {2009}
}

@inproceedings{Aschauer_2009,
 abstract = {Model-driven engineering of software intensive systems requires adequate means for describing their essential properties. For the domain of testbed automation systems, conventional modeling formalisms fall short due to the inadequacy of a fixed meta-level hierarchy. In this paper we identify the core problems by examining real-world examples. As a solution, we propose using a unification of classes and objects, known as clabjects. We propose extensions to the basic clabject notion for handling connector inheritance and instantiation, which are essential for bridging the gap between theoretical foundations and industrial applications.},
 author = {Thomas Aschauer and Gerd Dauenhauer and Wolfgang Pree},
 booktitle = {2009 35th Euromicro Conference on Software Engineering and Advanced Applications},
 doi = {10.1109/seaa.2009.46},
 publisher = {{IEEE}},
 title = {Multi-level Modeling for Industrial Automation Systems},
 url = {https://doi.org/10.1109%2Fseaa.2009.46},
 year = {2009}
}

@inproceedings{Atkinson_1997,
 abstract = {Meta-modeling is critical to the success of distributed object
environments such as CORBA and ActiveXI DCOM. However, there is a
surprisingly large variation in the nature of the meta-models (and
meta-meta-models) that have been proposed for such environments. This
paper investigates this phenomenon by examining the basic tenets of
meta-modeling in the context of distributed object environments, and by
defining the basic properties required of a suitable meta-modeling
framework. The paper is not concerned with the content of the
meta-models, per se, but rather with the form that this content should
take, and the rules that it should adhere to. The ramifications of these
rules on the notations and languages for distributed object environments
are then considered},
 author = {C. Atkinson},
 booktitle = {Proceedings First International Enterprise Distributed Object Computing Workshop},
 doi = {10.1109/edoc.1997.628350},
 publisher = {{IEEE}},
 title = {Meta-modelling for distributed object environments},
 url = {https://doi.org/10.1109%2Fedoc.1997.628350},
 year = {1997}
}

@article{Atkinson_2003,
 abstract = {},
 author = {C. Atkinson and T. Kuhne},
 doi = {10.1109/ms.2003.1231149},
 journal = {{IEEE} Software},
 month = {sep},
 number = {5},
 pages = {36--41},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Model-driven development: a metamodeling foundation},
 url = {https://doi.org/10.1109%2Fms.2003.1231149},
 volume = {20},
 year = {2003}
}

@article{Atkinson_2007,
 abstract = {A fundamental principle in engineering, including software engineering, is to minimize the amount of accidental complexity
which is introduced into engineering solutions due to mismatches between a problem and the technology used to represent the
problem. As model-driven development moves to the center stage of software engineering, it is particularly important that
this principle be applied to the technologies used to create and manipulate models, especially models that are intended to
be free of solution decisions. At present, however, there is a significant mismatch between the “two level” modeling paradigm
used to construct mainstream domain models and the conceptual information such models are required to represent—a mismatch
that makes such models more complex than they need be. In this paper, we identify the precise nature of the mismatch, discuss
a number of more or less satisfactory workarounds, and show how it can be avoided.},
 author = {Colin Atkinson and Thomas Kühne},
 doi = {10.1007/s10270-007-0061-0},
 journal = {Software {\&}amp$\mathsemicolon$ Systems Modeling},
 month = {jun},
 number = {3},
 pages = {345--359},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Reducing accidental complexity in domain models},
 url = {https://doi.org/10.1007%2Fs10270-007-0061-0},
 volume = {7},
 year = {2007}
}

@article{Berry_1995,
 abstract = {This paper examines a number of successful requirements engineering efforts carried out by the author and determines that a critical element in the success of these efforts was the author's ignorance of the client's domain},
 author = {Daniel M. Berry},
 doi = {10.1016/0164-1212(94)00054-q},
 journal = {Journal of Systems and Software},
 month = {feb},
 number = {2},
 pages = {179--184},
 publisher = {Elsevier {BV}},
 title = {The importance of ignorance in requirements engineering},
 url = {https://doi.org/10.1016%2F0164-1212%2894%2900054-q},
 volume = {28},
 year = {1995}
}

@article{Brooks_1987,
 abstract = {First Page of the Article},
 author = {Brooks},
 doi = {10.1109/mc.1987.1663532},
 journal = {Computer},
 month = {apr},
 number = {4},
 pages = {10--19},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {No Silver Bullet Essence and Accidents of Software Engineering},
 url = {https://doi.org/10.1109%2Fmc.1987.1663532},
 volume = {20},
 year = {1987}
}

@incollection{Gray_2007,
 abstract = {},
 author = {Jeff Gray and Sandeep Neema and Juha-Pekka Tolvanen and Aniruddha Gokhale and Steven Kelly and Jonathan Sprinkle},
 booktitle = {Chapman {\&}amp$\mathsemicolon$ Hall/{CRC} Computer {\&}amp$\mathsemicolon$ Information Science Series},
 doi = {10.1201/9781420010855.pt2},
 month = {jun},
 pages = {7--1--7--20},
 publisher = {Chapman and Hall/{CRC}},
 title = {Domain-Specific Modeling},
 url = {https://doi.org/10.1201%2F9781420010855.pt2},
 year = {2007}
}

@book{Kelly_2007,
 abstract = {"[The authors] are pioneers. . . . Few in our industry have their breadth of knowledge and experience." From the Foreword by Dave Thomas, Bedarra Labs 
Domain-Specific Modeling (DSM) is the latest approach to software development, promising to greatly increase the speed and ease of software creation. Early adopters of DSM have been enjoying productivity increases of 500-1000% in production for over a decade. This book introduces DSM and offers examples from various fields to illustrate to experienced developers how DSM can improve software development in their teams. Two authorities in the field explain what DSM is, why it works, and how to successfully create and use a DSM solution to improve productivity and quality. Divided into four parts, the book covers: background and motivation; fundamentals; in-depth examples; and creating DSM solutions. There is an emphasis throughout the book on practical guidelines for implementing DSM, including how to identify the necessary language constructs, how to generate full code from models, and how to provide tool support for a new DSM language. The example cases described in the book are available the book's website, www.dsmbook.com, along with an evaluation copy of the MetaEdit+ tool (for Windows, Mac OS X, and Linux), which allows readers to examine and try out the modeling languages and code generators. Domain-Specific Modeling is an essential reference for lead developers, software engineers, architects, methodologists, and technical managers who want to learn how to create a DSM solution and successfully put it into practice.},
 author = {Steven Kelly and Juha-Pekka Tolvanen},
 doi = {10.1002/9780470249260},
 month = {aug},
 publisher = {Wiley},
 title = {Domain-Specific Modeling},
 url = {https://doi.org/10.1002%2F9780470249260},
 year = {2007}
}

@article{Ledeczi_2001,
 abstract = {Domain-specific integrated development environments can help
capture specifications in the form of domain models. These tools support
the design process by automating analysis and simulating essential
system behavior. In addition, they can automatically generate,
configure, and integrate target application components. The high cost of
developing domain-specific, integrated modeling, analysis, and
application-generation environments prevents their penetration into
narrower engineering fields that have limited user bases.
Model-integrated computing (MIC), an approach to model-based engineering
that helps compose domain-specific design environments rapidly and cost
effectively, is particularly relevant for specialized computer-based
systems domains-perhaps even single projects. The authors describe how
MIC provides a way to compose such environments cost effectively and
rapidly by using a metalevel architecture to specify the domain-specific
modeling language and integrity constraints. They also discuss the
toolset that implements MIC and describe a practical application in
which using the technology in a tool environment for the process
industry led to significant reductions in development and maintenance
costs},
 author = {A. Ledeczi and A. Bakay and M. Maroti and P. Volgyesi and G. Nordstrom and J. Sprinkle and G. Karsai},
 doi = {10.1109/2.963443},
 journal = {Computer},
 number = {11},
 pages = {44--51},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Composing domain-specific design environments},
 url = {https://doi.org/10.1109%2F2.963443},
 volume = {34},
 year = {2001}
}

@article{Mernik_2005,
 abstract = {Domain-specific languages (DSLs) are languages tailored to a specific
application domain. They offer substantial gains in expressiveness
and ease of use compared with general-purpose programming languages
in their domain of application. DSL development is hard, requiring
both domain knowledge and language development expertise. Few people
have both. Not surprisingly, the decision to develop a DSL is often
postponed indefinitely, if considered at all, and most DSLs never
get beyond the application library stage.Although many articles have
been written on the development of particular DSLs, there is very
limited literature on DSL development methodologies and many questions
remain regarding when and how to develop a DSL. To aid the DSL developer,
we identify patterns in the decision, analysis, design, and implementation
phases of DSL development. Our patterns improve and extend earlier
work on DSL design patterns. We also discuss domain analysis tools
and language development systems that may help to speed up DSL development.
Finally, we present a number of open problems.},
 author = {Marjan Mernik and Jan Heering and Anthony M. Sloane},
 doi = {10.1145/1118890.1118892},
 journal = {{ACM} Computing Surveys},
 month = {dec},
 number = {4},
 pages = {316--344},
 publisher = {Association for Computing Machinery ({ACM})},
 title = {When and how to develop domain-specific languages},
 url = {https://doi.org/10.1145%2F1118890.1118892},
 volume = {37},
 year = {2005}
}

@article{Niu_2007,
 abstract = {},
 author = {Nan Niu and Steve Easterbrook},
 doi = {10.1109/ms.2007.52},
 journal = {{IEEE} Software},
 month = {mar},
 number = {2},
 pages = {53--61},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {So, You Think You Know Others{\textquotesingle} Goals? A Repertory Grid Study},
 url = {https://doi.org/10.1109%2Fms.2007.52},
 volume = {24},
 year = {2007}
}

@incollection{Nordstrom_2003,
 abstract = {A domain-specific language provides domain experts with a familiar abstraction for creating computer programs. As more and more domains embrace computers, programmers are tapping into this power by creating their own languages fitting the particular needs of the domain. Graphical domain-specific modeling languages are even more appealing for non-programmers, since the modeling language constructs are automatically transformed into applications through a special compiler called a translator. The Generic Modeling Environment (GME) at Vanderbilt University is a meta-programmable modeling environment. Translators written to interface with GME models typically use a domain-independent APT. This paper presents a tool called ANEMIC that generates a domain-specific APT for GME translators using the same metamodel that generates the language.},
 author = {Steve Nordstrom and Shweta Shetty and Kumar Gaurav Chhokra and Jonathan Sprinkle and Brandon Eames and Akos Ledeczi},
 booktitle = {Generative Programming and Component Engineering},
 doi = {10.1007/978-3-540-39815-8_9},
 pages = {138--150},
 publisher = {Springer Berlin Heidelberg},
 title = {{ANEMIC}: Automatic Interface Enabler for Model Integrated Computing},
 url = {https://doi.org/10.1007%2F978-3-540-39815-8_9},
 year = {2003}
}

@inproceedings{Parnas,
 abstract = {Programs, like people, get old. We can't prevent aging, but we can understand its causes, take steps to limits its effects, temporarily reverse some of the damage it has caused, and prepare for the day when the software is no longer viable. A sign that the software engineering profession has matured will be that we lose our preoccupation with the first release and focus on the long-term health of our products. Researchers and practitioners must change their perception of the problems of software development. Only then will software engineering deserve to be called “engineering”},
 author = {D.L. Parnas},
 booktitle = {Proceedings of 16th International Conference on Software Engineering},
 doi = {10.1109/icse.1994.296790},
 publisher = {{IEEE} Comput. Soc. Press},
 title = {Software aging},
 url = {https://doi.org/10.1109%2Ficse.1994.296790}
}

@article{Pfleeger_1999,
 abstract = {Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.},
 author = {S.L Pfleeger},
 doi = {10.1016/s0164-1212(99)00031-x},
 journal = {Journal of Systems and Software},
 month = {jul},
 number = {2-3},
 pages = {111--124},
 publisher = {Elsevier {BV}},
 title = {Understanding and improving technology transfer in software engineering},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900031-x},
 volume = {47},
 year = {1999}
}

@article{Ramos_2005,
 abstract = {Traditional approaches to requirements elicitation stress systematic and rational analysis and representation of organizational context and system requirements. This paper argues that the introduction of any computer-based system to an organization transforms the organization and changes the work patterns of the system's users in the organization. These changes interact with the users' values and beliefs and trigger emotional responses which are sometimes directed against the computer-based system and its proponents. The paper debunks myths about how smoothly such organizational transformations take place, describes case studies showing how organizational transformation really takes place, and introduces and confirms by case studies some guidelines for eliciting requirements and the relevant emotional issues for a computer-based system that is being introduced into an organization to change its work patterns.},
 author = {Isabel Ramos and Daniel M. Berry and Jo{\~{a}}o {\'{A}}. Carvalho},
 doi = {10.1016/j.infsof.2004.09.014},
 journal = {Information and Software Technology},
 month = {may},
 number = {7},
 pages = {479--495},
 publisher = {Elsevier {BV}},
 title = {Requirements engineering for organizational transformation},
 url = {https://doi.org/10.1016%2Fj.infsof.2004.09.014},
 volume = {47},
 year = {2005}
}

@inproceedings{Rombach_2007,
 abstract = {The rapid and complex research and innovation processes require high-tech companies to optimize their technology transfer processes. It is clearly not sufficient to solely rely on internal R&D; strategic cooperations with external research centers of excellence are needed in order to compete in the global innovation market. Candidates for such strategic cooperations are universities, research institutions, and technology focused consulting companies. Key challenge is the effective integration of external competences into the company-internal innovation processes. In this paper we present a survey of the state-of-the-art in technology transfer, high-light promising success cases for the future, and derive success criteria for successful technology transfer in a global world. The cooperation between Siemens and Fraunhofer IESE is presented as a concrete example.},
 author = {Dieter Rombach and Reinhold Achatz},
 booktitle = {Future of Software Engineering ({FOSE} {\textquotesingle}07)},
 doi = {10.1109/fose.2007.16},
 month = {may},
 publisher = {{IEEE}},
 title = {Research Collaborations between Academia and Industry},
 url = {https://doi.org/10.1109%2Ffose.2007.16},
 year = {2007}
}

@article{Taivalsaari_1996,
 abstract = {One of the most intriguing—and at the same time most problematic—notions in object-oriented programing is inheritance. Inheritance is commonly regarded as the feature that distinguishes object-oriented programming from other modern programming paradigms, but researchers rarely agree on its meaning and usage. Yet inheritance of often hailed as a solution to many problems hampering software development, and many of the alleged benefits of object-oriented programming, such as improved conceptual modeling and reusability, are largely credited to it. This article aims at a comprehensive understanding of inheritance, examining its usage, surveying its varieties, and presenting a simple taxonomy of mechanisms that can be seen as underlying different inheritance models.},
 author = {Antero Taivalsaari},
 doi = {10.1145/243439.243441},
 journal = {{ACM} Computing Surveys},
 month = {sep},
 number = {3},
 pages = {438--479},
 publisher = {Association for Computing Machinery ({ACM})},
 title = {On the notion of inheritance},
 url = {https://doi.org/10.1145%2F243439.243441},
 volume = {28},
 year = {1996}
}

@inproceedings{Ungar_1987,
 abstract = {},
 author = {David Ungar and Randall B. Smith},
 booktitle = {Conference proceedings on Object-oriented programming systems, languages and applications  - {OOPSLA} {\textquotesingle}87},
 doi = {10.1145/38765.38828},
 publisher = {{ACM} Press},
 title = {Self: The power of simplicity},
 url = {https://doi.org/10.1145%2F38765.38828},
 year = {1987}
}

@inproceedings{Ungar_2007,
 abstract = {},
 author = {David Ungar and Randall B. Smith},
 booktitle = {Proceedings of the third {ACM} {SIGPLAN} conference on History of programming languages},
 doi = {10.1145/1238844.1238853},
 month = {jun},
 publisher = {{ACM}},
 title = {Self},
 url = {https://doi.org/10.1145%2F1238844.1238853},
 year = {2007}
}

@inproceedings{Vajk_2009,
 abstract = {Model-based development methodologies are gaining ground as software applications are getting more and more complex while the pressure to decrease time-to-market continually increase. Domain-specific modeling tools that support system analysis, simulation, and automatic code generation can increase productivity. However, most domain-specific model translators are still manually written. This paper presents a technique that automatically generates a domain-specific application programming interface from the same metamodels that are used to define the domain-specific modeling language itself. This facilitates the creation of domain-specific model translators by providing a high-level abstraction hiding all the cumbersome modeling tool-specific implementation details from the developer. The approach is illustrated using the generic modeling environment and the Microsoft .NET C# language.},
 author = {Tam{\'{a}}s Vajk and R{\'{o}}bert Keresk{\'{e}}nyi and Tiham{\'{e}}r Levendovszky and {\'{A}}kos L{\'{e}}deczi},
 booktitle = {2009 16th Annual {IEEE} International Conference and Workshop on the Engineering of Computer Based Systems},
 doi = {10.1109/ecbs.2009.30},
 month = {apr},
 publisher = {{IEEE}},
 title = {Raising the Abstraction of Domain-Specific Model Translator Development},
 url = {https://doi.org/10.1109%2Fecbs.2009.30},
 year = {2009}
}

@inproceedings{Wile_2003,
 abstract = {},
 author = {D. Wile},
 booktitle = {36th Annual Hawaii International Conference on System Sciences, 2003. Proceedings of the},
 doi = {10.1109/hicss.2003.1174893},
 publisher = {{IEEE}},
 title = {Lessons learned from real {DSL} experiments},
 url = {https://doi.org/10.1109%2Fhicss.2003.1174893},
 year = {2003}
}

@article{2001,
 abstract = {},
 doi = {10.5860/choice.38-5616},
 journal = {Choice Reviews Online},
 month = {jun},
 number = {10},
 pages = {38--5616--38--5616},
 publisher = {American Library Association},
 title = {Bootstrapping: Douglas Engelbart, coevolution, and the origins of personal computing},
 url = {https://doi.org/10.5860%2Fchoice.38-5616},
 volume = {38},
 year = {2001}
}

@inproceedings{Aoyama_2002,
 abstract = {This article discusses continuous and discontinuous evolutions across multiple products lines of software systems. As an empirical study, we analyzed the evolution of mobile phone software systems, which have undergone an extremely fast evolution from voice communication systems through mobile Internet terminals, to mobile Javaenabled terminals for the last four years. Along with the evolution, the software systems created three product lines and the size has quadrupled. This article discusses aspects of such extremely fast evolution and reveals continuity and discontinuity of software evolution. We claim that discontinuity is essential aspect of software evolution and taming discontinuous evolution is the challenge in the evolutionary development of software systems.},
 author = {Mikio Aoyama},
 booktitle = {Proceedings of the 4th international workshop on Principles of software evolution  - {IWPSE} {\textquotesingle}01},
 doi = {10.1145/602461.602477},
 publisher = {{ACM} Press},
 title = {Continuous and discontinuous software evolution},
 url = {https://doi.org/10.1145%2F602461.602477},
 year = {2002}
}

@article{Jones_1995,
 abstract = {If you were the vice president of software in a company with
10,000 software personnel, what would you do to make sure your software
team had state-of-the-art tools and methodologies? At a more fundamental
level, how would you and your staff even find out what they are and
whether your current tools and methodologies are good, bad, or average?
That is the crux of two major challenges to the software community: How
do we evaluate tools and methods for effectiveness? How do we deploy
better tools and methods once they have been identified? Unfortunately,
the software industry lacks standard measurements and benchmarks for
evaluating the effectiveness of programming tools and languages, design
approaches, or almost any other kind of technology. Purchasing and
acquisition decisions are often made on the basis of unsubstantiated
vendor claims. Moreover, once a new tool or methodology is acquired,
deployment is often slow. Tools are acquired without considering
training needs, or if training is considered, it's not readily available
due to schedule pressures},
 author = {C. Jones},
 doi = {10.1109/2.386991},
 journal = {Computer},
 month = {jun},
 number = {6},
 pages = {86--87},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Why is technology transfer so hard?},
 url = {https://doi.org/10.1109%2F2.386991},
 volume = {28},
 year = {1995}
}

@article{Lewin_1999,
 abstract = {This paper outlines an alternative theory of organization–en-vironment coevolution that generalizes a model of organization adaptation first proposed by March (1991), linking firm-level exploration and exploitation adaptations to changes in the popu-lation of organizations. The theory considers organizations, their populations, and their environments as the interdependent outcome of managerial actions, institutional influences, and extra-institutional changes (technological, sociopolitical, and other environmental phenomena). In particular, the theory in-corporates potential differences and equifinal outcomes related to country-specific variation. The basic theses of this paper are that firm strategic and organization adaptations coevolve with changes in the environment (competitive dynamics, technolog-ical, and institutional) and organization population and forms, and that new organizational forms can mutate and emerge from the existing population of organizations. The theory has guided a multicountry research collaboration on strategic and organi-zation adaptations and the mutation and emergence of new or-ganizational forms from within the existing population of or-ganizations.},
 author = {Arie Y. Lewin and Chris P. Long and Timothy N. Carroll},
 doi = {10.1287/orsc.10.5.535},
 journal = {Organization Science},
 month = {oct},
 number = {5},
 pages = {535--550},
 publisher = {Institute for Operations Research and the Management Sciences ({INFORMS})},
 title = {The Coevolution of New Organizational Forms},
 url = {https://doi.org/10.1287%2Forsc.10.5.535},
 volume = {10},
 year = {1999}
}

@inproceedings{Rombach_2000,
 abstract = {The Fraunhofer Gesellschaft e.V. in Germany is Europe's largest and most successful organization for applied research and technology transfer. Its 48 institutes cover all areas of technology and engineering ranging from materials and production technology to information & communication technology and solar energy. The Fraunhofer Institute for Experimental Software Engineering (IESE) in Kaiserslautern, Germany, focuses on software engineering methods, software product and process management, and learning organization concepts for software. It applies an experiment- or feedback-based transfer model, which has led to many successful and sustained improvements in the industrial practice of software development. In this presentation, the underlying transfer model, key business areas and core competencies of Fraunhofer IESE as well as examples of industrial transfer projects will be illustrated. The presentation will conclude with arguments why this transfer approach is well suited for software development and why it is a prerequisite for the professionalization of software development},
 author = {Dieter Rombach},
 booktitle = {Proceedings of the 22nd international conference on Software engineering  - {ICSE} {\textquotesingle}00},
 doi = {10.1145/337180.337443},
 publisher = {{ACM} Press},
 title = {Fraunhofer},
 url = {https://doi.org/10.1145%2F337180.337443},
 year = {2000}
}

@article{Shaw_1990,
 abstract = {Although software engineering is not yet a true engineering discipline, it has the potential to become one. Older engineering fields are examined to ascertain the character that software engineering might have. The current state of software technology is discussed, covering information processing as an economic force, the growing role of software in critical applications, the maturity of development techniques, and the scientific basis for software engineering practice. Five basic steps that the software engineering profession must take to become a true engineering discipline are described. They are: understanding the nature of expertise, recognizing different ways to get information, encouraging routine practice, expecting professional specializations, and improving the coupling between science and commercial practice.< >},
 author = {M. Shaw},
 doi = {10.1109/52.60586},
 journal = {{IEEE} Software},
 month = {nov},
 number = {6},
 pages = {15--24},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Prospects for an engineering discipline of software},
 url = {https://doi.org/10.1109%2F52.60586},
 volume = {7},
 year = {1990}
}

@article{Stubrich_1993,
 abstract = {},
 author = {Gustavo Stubrich},
 doi = {10.1016/0022-5428(93)90045-q},
 journal = {The Columbia Journal of World Business},
 month = {jun},
 number = {2},
 pages = {108--109},
 publisher = {Elsevier {BV}},
 title = {},
 url = {https://doi.org/10.1016%2F0022-5428%2893%2990045-q},
 volume = {28},
 year = {1993}
}

@article{Anquetil_1999,
 abstract = {},
 author = {Nicolas Anquetil and Timothy C. Lethbridge},
 doi = {10.1002/(sici)1096-908x(199905/06)11:3<201::aid-smr192>3.0.co;2-1},
 journal = {Journal of Software Maintenance: Research and Practice},
 month = {may},
 number = {3},
 pages = {201--221},
 publisher = {Wiley},
 title = {Recovering software architecture from the names of source files},
 url = {https://doi.org/10.1002%2F%28sici%291096-908x%28199905%2F06%2911%3A3%3C201%3A%3Aaid-smr192%3E3.0.co%3B2-1},
 volume = {11},
 year = {1999}
}

@article{Anquetil_2003,
 abstract = {As valuable software systems become older, reverse engineering becomes increasingly important to companies that have to maintain the code. Clustering is a key activity in reverse engineering that is used to discover improved designs of systems or to extract significant concepts from code. Clustering is an old, highly sophisticated, activity which offers many methods to meet different needs. The various methods have been well documented in the past, however conclusions from general clustering literature may not apply entirely to the reverse engineering domain. In this paper, we study three decisions that need to be made when clustering: the choice of 1) abstract descriptions of the entities to be clustered, 2) metrics to compute coupling between the entities, and 3) clustering algorithms. For each decision, our objective is to understand which choices are best when performing software remodularization. The experiments were conducted on three public domain systems (gcc, linux and mosaic) and a real world legacy system (2 million loc). Among other things, we confirm the importance of a proper description scheme for the entities being clustered, we list a few effective coupling metrics and characterize the quality of different clustering algorithms. We also propose novel description schemes not directly based on the source code, and we advocate better formal evaluation methods for the clustering results.},
 author = {N. Anquetil and T.C. Lethbridge},
 doi = {10.1049/ip-sen:20030581},
 journal = {{IEE} Proceedings - Software},
 number = {3},
 pages = {185},
 publisher = {Institution of Engineering and Technology ({IET})},
 title = {Comparative study of clustering algorithms and abstract representations for software remodularisation},
 url = {https://doi.org/10.1049%2Fip-sen%3A20030581},
 volume = {150},
 year = {2003}
}

@article{Baskerville_1996,
 abstract = {This paper reviews the origins, techniques and roles associated with action research into information systems (IS). Many consider the approach to be the paragon of post-positivist research methods, yet it has a cloudy history among the social sciences. The paper summarizes the rigorous approach to action research and suggests certain domains of ideal use (such as systems development methodology). For those faced with conducting, reviewing or examining action research, the paper discusses various problems, opportunities and strategies.},
 author = {Richard L. Baskerville and A. Trevor Wood-Harper},
 doi = {10.1177/026839629601100305},
 journal = {Journal of Information Technology},
 month = {sep},
 number = {3},
 pages = {235--246},
 publisher = {{SAGE} Publications},
 title = {A Critical Perspective on Action Research as a Method for Information Systems Research},
 url = {https://doi.org/10.1177%2F026839629601100305},
 volume = {11},
 year = {1996}
}

@article{Beckman_1997,
 abstract = {When it comes to software engineering education, there is a gap
between what industry needs and what universities offer. To close this
gap, the authors propose a comprehensive collaboration between academic
software engineering programs and industry. They offer a model for this
collaboration and highlight three real-world ventures},
 author = {K. Beckman and N. Coulter and S. Khajenoori and N.R. Mead},
 doi = {10.1109/52.636668},
 journal = {{IEEE} Software},
 number = {6},
 pages = {49--57},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Collaborations: closing the industry-academia gap},
 url = {https://doi.org/10.1109%2F52.636668},
 volume = {14},
 year = {1997}
}

@incollection{Conradi_2003,
 abstract = {Software development is an experimental discipline, i.e. somewhat unpredictable. This suggests that software processes improvement should be based on the continuous iteration of characterization, goal setting, selection of improved technology, monitoring and analysis of its effects. This paper describes experiences from the empirical studies in two large SPI programmes in Norway. Five main lessons were learned: 1) It is a challenge for the industrial partners to invest enough reso urces in SPI activities. 2) The research partners must learn to know the companies, and 3) they must work as a multi- competent and coherent unit towards them. 4) Any SPI initiative must show visible, short-term payoff. 5) Establishing a solid baseline from which to improve is unrealistic. Based on these lessons, a set of operational recommendations for other researchers in the area are proposed.},
 author = {Reidar Conradi and Tore Dyb{\aa} and Dag I. K. Sj{\o}berg and Tor Ulsund},
 booktitle = {Software Process Technology},
 doi = {10.1007/978-3-540-45189-1_4},
 pages = {32--45},
 publisher = {Springer Berlin Heidelberg},
 title = {Lessons Learned and Recommendations from Two Large Norwegian {SPI} Programmes},
 url = {https://doi.org/10.1007%2F978-3-540-45189-1_4},
 year = {2003}
}

@article{https://doi.org/10.20381/ruor-14885,
 abstract = {},
 author = {Lethbridge, Timothy Christian.},
 doi = {10.20381/RUOR-14885},
 keywords = {Information Science.},
 publisher = {Université d'Ottawa / University of Ottawa},
 title = {Practical techniques for organizing and measuring knowledge.},
 url = {http://ruor.uottawa.ca/handle/10393/6535},
 year = {1994}
}

@inproceedings{Jelber_Sayyad_Shirabad,
 abstract = {A considerable amount of system maintenance experience can be found in bug tracking and source code configuration management systems. Data mining and machine learning techniques allow one to extract models from past experience that can be used in future predictions. By mining the software change record, one can therefore generate models that can be used in future maintenance activities. In this paper, we present an example of such a model that represents a relation between pairs of files and show how it can be extracted from the software update records of a real world legacy system. We show how different sources of data can be used to extract sets of features useful in describing this model, as well as how results are affected by these different feature sets and their combinations. Our best results were obtained from text-based features, i.e. those extracted from words in the problem reports as opposed to syntactic structures in the source code.},
 author = {Jelber Sayyad Shirabad and T.C. Lethbridge and S. Matwin},
 booktitle = {International Conference on Software Maintenance, 2003. {ICSM} 2003. Proceedings.},
 doi = {10.1109/icsm.2003.1235410},
 publisher = {{IEEE} Comput. Soc},
 title = {Mining the maintenance history of a legacy software system},
 url = {https://doi.org/10.1109%2Ficsm.2003.1235410}
}

@article{Lethbridge_2005,
 abstract = {Software engineering is an intensely people-oriented activity, yet too little is known about how designers, maintainers, requirements analysts and all other types of software engineers perform their work. In order to improve software engineering tools and practice, it is therefore essential to conduct field studies, i.e., to study real practitioners as they solve real problems. To do so effectively, however, requires an understanding of the techniques most suited to each type of field study task. In this paper, we provide a taxonomy of techniques, focusing on those for data collection. The taxonomy is organized according to the degree of human intervention each requires. For each technique, we provide examples from the literature, an analysis of some of its ad- vantages and disadvantages, and a discussion of how to use it effectively. We also briefly talk about field study design in general, and data analysis.},
 author = {Timothy C. Lethbridge and Susan Elliott Sim and Janice Singer},
 doi = {10.1007/s10664-005-1290-x},
 journal = {Empirical Software Engineering},
 month = {jul},
 number = {3},
 pages = {311--341},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Studying Software Engineers: Data Collection Techniques for Software Field Studies},
 url = {https://doi.org/10.1007%2Fs10664-005-1290-x},
 volume = {10},
 year = {2005}
}

@article{Liu_2002,
 abstract = {This paper describes a study of what we call intelligent search techniques as implemented in a software maintenance environment. The techniques studied include abbreviation concatenation and abbreviation expansion. We also describe rating algorithms used to prioritize the query results. To evaluate our approach, we present a series of experiments in which we compare our algorithms' ratings of results to ratings provided by software engineers.},
 author = {Huixiang Liu and Timothy C. Lethbridge},
 doi = {10.1023/a:1020839810474},
 journal = {Information Systems Frontiers},
 number = {4},
 pages = {409--423},
 publisher = {Springer Science and Business Media {LLC}},
 url = {https://doi.org/10.1023%2Fa%3A1020839810474},
 volume = {4},
 year = {2002}
}

@article{Mead_1999,
 abstract = {In this paper, we present the results of a survey of formal industry/university collaborations. The purpose of these collaborations is to meet the software engineering education and training needs of adult learners through joint ventures such as graduate programs (degree and certificate) and professional development activities (customized classes, seminars, forums, and conferences). Members of the Software Engineering Institute (SEI) working group on software engineering education and training conducted the survey in 1997–1998. The working group drew on the extensive experience of industry and university collaboration participants to help answer practical questions about the benefits of collaboration, the collaboration process itself, successful collaboration administration and programming, and lessons learned. Survey results are being published as a service to the software engineering education and training community to assist organizations interested in forming a new collaboration or improving an existing collaboration.},
 author = {Nancy Mead and Kathy Beckman and Jimmy Lawrence and George O'Mary and Cynthia Parish and Perla Unpingco and Hope Walker},
 doi = {10.1016/s0164-1212(99)00091-6},
 journal = {Journal of Systems and Software},
 month = {dec},
 number = {2-3},
 pages = {155--162},
 publisher = {Elsevier {BV}},
 title = {Industry/university collaborations: different perspectives heighten mutual opportunities},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900091-6},
 volume = {49},
 year = {1999}
}

@article{Pfleeger_1999,
 abstract = {Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.},
 author = {S.L Pfleeger},
 doi = {10.1016/s0164-1212(99)00031-x},
 journal = {Journal of Systems and Software},
 month = {jul},
 number = {2-3},
 pages = {111--124},
 publisher = {Elsevier {BV}},
 title = {Understanding and improving technology transfer in software engineering},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900031-x},
 volume = {47},
 year = {1999}
}

@article{Potts_1993,
 abstract = {The author discusses three major changes that he suggests are occurring as a result of the software engineering industry adopting the industry-as-laboratory approach, in which researchers identify problems through close involvement with industrial projects and create and evaluate solutions in an almost indivisible research activity. This approach emphasizes what people actually do or can do in practice, rather than what is possible in principle. The three changes are a greater reliance on empirical definition of problems, an emphasis on real case studies, and a greater emphasis on contextual issues.< >},
 author = {C. Potts},
 doi = {10.1109/52.232392},
 journal = {{IEEE} Software},
 month = {sep},
 number = {5},
 pages = {19--28},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Software-engineering research revisited},
 url = {https://doi.org/10.1109%2F52.232392},
 volume = {10},
 year = {1993}
}

@article{Sjoeberg_2005,
 abstract = {The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.},
 author = {D.I.K. Sjoeberg and J.E. Hannay and O. Hansen and V.B. Kampenes and A. Karahasanovic and N.-K. Liborg and A.C. Rekdal},
 doi = {10.1109/tse.2005.97},
 journal = {{IEEE} Transactions on Software Engineering},
 month = {sep},
 number = {9},
 pages = {733--753},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {A survey of controlled experiments in software engineering},
 url = {https://doi.org/10.1109%2Ftse.2005.97},
 volume = {31},
 year = {2005}
}

@article{Williams_2005,
 abstract = {A goal of evidence-based software engineering is to provide a means by which industry practitioners can make rational decisions about technology adoption. When a technology is mature enough for potential widespread use, practitioners find empirical evidence most compelling when the study has taken place in a live, industrial situation in an environment comparable to their own. However, empirical software engineering is in need of guidelines and standards to direct industrial case studies so that the results of this research are valuable and can be combined into an evidentiary base. In this paper, we present a high-level view of a measurement framework that has been used with multiple agile software development industrial case studies. We propose that this technology-dependent framework can be used as a strawman for a guideline of data collection, analysis, and reporting of industrial case studies. Our goal in offering the framework as a strawman is to solicit input from the community on a guideline for the essential components of a technology-dependent framework for industrial case study research.},
 author = {Laurie Williams and Lucas Layman and Pekka Abrahamsson},
 doi = {10.1145/1082983.1083179},
 journal = {{ACM} {SIGSOFT} Software Engineering Notes},
 month = {may},
 number = {4},
 pages = {1--5},
 publisher = {Association for Computing Machinery ({ACM})},
 title = {On establishing the essential components of a technology-dependent framework},
 url = {https://doi.org/10.1145%2F1082983.1083179},
 volume = {30},
 year = {2005}
}

@article{1995,
 abstract = {},
 doi = {10.1016/0967-0661(95)90062-4},
 journal = {Control Engineering Practice},
 month = {dec},
 number = {12},
 pages = {1787--1788},
 publisher = {Elsevier {BV}},
 title = {A discipline for software engineering},
 url = {https://doi.org/10.1016%2F0967-0661%2895%2990062-4},
 volume = {3},
 year = {1995}
}

@incollection{Basili,
 abstract = {},
 author = {Victor R. Basili and Gianluigi Caldiera},
 booktitle = {Foundations of Empirical Software Engineering},
 doi = {10.1007/3-540-27662-9_16},
 pages = {220--237},
 publisher = {Springer Berlin Heidelberg},
 title = {Improve Software Quality by Reusing Knowledge and Experience},
 url = {https://doi.org/10.1007%2F3-540-27662-9_16}
}

@article{Basili_1986,
 abstract = {Experimentation in software engineering supports the advancement of the field through an iterative learning process. In this paper, a framework for analyzing most of the experimental work performed in software engineering over the past several years is presented. A variety of experiments in the framework is described and their contribution to the software engineering discipline is discussed. Some useful recommendations for the application of the experimental process in software engineering are included.},
 author = {Victor R. Basili and Richard W. Selby and David H. Hutchens},
 doi = {10.1109/tse.1986.6312975},
 journal = {{IEEE} Transactions on Software Engineering},
 month = {jul},
 number = {7},
 pages = {733--743},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Experimentation in software engineering},
 url = {https://doi.org/10.1109%2Ftse.1986.6312975},
 volume = {{SE}-12},
 year = {1986}
}

@article{Basili_1987,
 abstract = {This study applies an experimentation methodology to compare three state-of-the-practice software testing techniques: a) code reading by stepwise abstraction, b) functional testing using equivalence partitioning and boundary value analysis, and c) structural testing using 100 percent statement coverage criteria. The study compares the strategies in three aspects of software testing: fault detection effectiveness, fault detection cost, and classes of faults detected. Thirty-two professional programmers and 42 advanced students applied the three techniques to four unit-sized programs in a fractional factorial experimental design. The major results of this study are the following. 1) With the professional programmers, code reading detected more software faults and had a higher fault detection rate than did functional or structural testing, while functional testing detected more faults than did structural testing, but functional and structural testing were not different in fault detection rate. 2) In one advanced student subject group, code reading and functional testing were not different in faults found, but were both superior to structural testing, while in the other advanced student subject group there was no difference among the techniques. 3) With the advanced student subjects, the three techniques were not different in fault detection rate. 4) Number of faults observed, fault detection rate, and total effort in detection depended on the type of software tested. 5) Code reading detected more interface faults than did the other methods. 6) Functional testing detected more control faults than did the other methods.},
 author = {V.R. Basili and R.W. Selby},
 doi = {10.1109/tse.1987.232881},
 journal = {{IEEE} Transactions on Software Engineering},
 month = {dec},
 number = {12},
 pages = {1278--1296},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Comparing the Effectiveness of Software Testing Strategies},
 url = {https://doi.org/10.1109%2Ftse.1987.232881},
 volume = {{SE}-13},
 year = {1987}
}

@inproceedings{Basili_1992,
 abstract = {},
 author = {Victor Basili and Gianluigi Caldiera and Frank McGarry and Rose Pajerski and Gerald Page and Sharon Waligora},
 booktitle = {Proceedings of the 14th international conference on Software engineering  - {ICSE} {\textquotesingle}92},
 doi = {10.1145/143062.143154},
 publisher = {{ACM} Press},
 title = {The software engineering laboratory},
 url = {https://doi.org/10.1145%2F143062.143154},
 year = {1992}
}

@incollection{Basili_2000,
 abstract = {Without Abstract},
 author = {Victor R. Basili},
 booktitle = {Software Process Technology},
 doi = {10.1007/bfb0095023},
 pages = {150--150},
 publisher = {Springer Berlin Heidelberg},
 title = {Keynote on {\textquotedblleft}Experimental software engineering{\textquotedblright}},
 url = {https://doi.org/10.1007%2Fbfb0095023},
 year = {2000}
}

@inproceedings{Basili_2002,
 abstract = {For 25 years the NASA/GSFC Software Engineering Laboratory (SEL) has been a major resource in software process improvement activities. But due to a changing climate at NASA, agency reorganization, and budget cuts, the SEL has lost much of its impact. In this paper we describe the history of the SEL and give some lessons learned on what we did right, what we did wrong, and what others can learn from our experiences. We briefly describe the research that was conducted by the SEL, describe how we evolved our understanding of software process improvement, and provide a set of lessons learned and hypotheses that should enable future groups to learn from and improve on our quarter century of experiences.},
 author = {Victor R. Basili and Frank E. McGarry and Rose Pajerski and Marvin V. Zelkowitz},
 booktitle = {Proceedings of the 24th international conference on Software engineering  - {ICSE} {\textquotesingle}02},
 doi = {10.1145/581339.581351},
 publisher = {{ACM} Press},
 title = {Lessons learned from 25 years of process improvement},
 url = {https://doi.org/10.1145%2F581339.581351},
 year = {2002}
}

@inproceedings{Houdek,
 abstract = {The experience factory concept enables systematic learning and continuous improvement in software development. As with most learning initiatives, it is hard to establish. In our experience, there is a great deal of uncertainty and skepticism about the mission and contents of an experience factory. The starting phase is especially endangered through pitfalls or unexpected delays. As expectations vary and there is pressure to demonstrate success within only a few months, tension arises which may jeopardize the entire enterprise. In the course of a large-scale software improvement program, we have established three experience factories in different environments of the Daimler-Benz AG within two years. At each site, several application projects are involved. In this paper we describe how we approached the task, what actions we took, and the lessons we learned.},
 author = {F. Houdek and K. Schneider and E. Wieser},
 booktitle = {Proceedings of the 20th International Conference on Software Engineering},
 doi = {10.1109/icse.1998.671602},
 publisher = {{IEEE} Comput. Soc},
 title = {Establishing experience factories at Daimler-Benz an experience report},
 url = {https://doi.org/10.1109%2Ficse.1998.671602}
}

@book{Juristo_2001,
 abstract = {Basics of Software Engineering Experimentation is a practical guide to experimentation in a field which has long been underpinned by suppositions, assumptions, speculations and beliefs. It demonstrates to software engineers how Experimental Design and Analysis can be used to validate their beliefs and ideas. The book does not assume its readers have an in-depth knowledge of mathematics, specifying the conceptual essence of the techniques to use in the design and analysis of experiments and keeping the mathematical calculations clear and simple. Basics of Software Engineering Experimentation is practically oriented and is specially written for software engineers, all the examples being based on real and fictitious software engineering experiments.},
 author = {Natalia Juristo and Ana M. Moreno},
 doi = {10.1007/978-1-4757-3304-4},
 publisher = {Springer {US}},
 title = {Basics of Software Engineering Experimentation},
 url = {https://doi.org/10.1007%2F978-1-4757-3304-4},
 year = {2001}
}

@incollection{Kamsties_1995,
 abstract = {We replicated a controlled experiment first run in the early 1980's to evaluate the effectiveness and efficiency of 50 student subjects who used three defectdetection techniques to observe failures and isolate faults in small C programs. The three techniques were code reading by stepwise abstraction, functional (black-box) testing, and structural (white-box) testing. Two internal replications showed that our relatively inexperienced subjects were similarly effective at observing failures and isolating faults with all three techniques. However, our subjects were most efficient at both tasks when they used functional testing. Some significant differences among the techniques in their effectiveness at isolating faults of different types were seen. These results suggest that inexperienced subjects can apply a formal verification technique (code reading) as effectively as an execution-based validation technique, but they are most efficient when using functional testing. 1 Introduction Sof...},
 author = {Erik Kamsties and Christopher M. Lott},
 booktitle = {Software Engineering {\textemdash} {ESEC} {\textquotesingle}95},
 doi = {10.1007/3-540-60406-5_25},
 pages = {362--383},
 publisher = {Springer Berlin Heidelberg},
 title = {An empirical evaluation of three defect-detection techniques},
 url = {https://doi.org/10.1007%2F3-540-60406-5_25},
 year = {1995}
}

@article{LANDES_1999,
 abstract = {Learning from experiences in the software domain is an important issue for the DaimlerChrysler Corporation. Unfortunately, there are no textbook recipes on how a process of organizational learning can be established. In particular, those types of experiences must be identified that are potentially valuable for reuse. Furthermore, the organization and representation of such experiences must be defined in such a way that they can easily be retrieved and used for the solving of new problems. In this paper, we provide some insights that we gained during the examination of these issues in projects aiming at establishing a so-called experience factory.},
 author = {DIETER LANDES and KURT SCHNEIDER and FRANK HOUDEK},
 doi = {10.1006/ijhc.1999.0280},
 journal = {International Journal of Human-Computer Studies},
 month = {sep},
 number = {3},
 pages = {643--661},
 publisher = {Elsevier {BV}},
 title = {Organizational learning and experience documentation in industrial software projects},
 url = {https://doi.org/10.1006%2Fijhc.1999.0280},
 volume = {51},
 year = {1999}
}

@article{Miller_1997,
 abstract = {Recently we have witnessed a welcomed increase in the amount of empirical evaluation of Software Engineering methods and concepts. It is hoped that this increase will lead to establishing Software Engineering as a well-defined subject with a sound scientifically proven underpinning rather than a topic based upon unsubstantiated theories and personal belief. For this to happen the empirical work must be of the highest standard. Unfortunately producing meaningful empirical evaluations is a highly hazardous activity, full of uncertainties and often unseen difficulties. Any researcher can overlook or neglect a seemingly innocuous factor, which in fact invalidates all of the work. More serious is that large sections of the community can overlook essential experimental design guidelines, which bring into question the validity of much of the work undertaken to date.In this paper, the authors address one such factor — Statistical Power Analysis. It is believed, and will be demonstrated, that any body of research undertaken without considering statistical power as a fundamental design parameter is potentially fatally flawed. Unfortunately the authors are unaware of much Software Engineering research which takes this parameter into account. In addition to introducing Statistical Power, the paper will attempt to demonstrate the potential difficulties of applying it to the design of Software Engineering experiments and concludes with a discussion of what the authors believe is the most viable method of incorporating the evaluation of statistical power within the experimental design process.},
 author = {James Miller and John Daly and Murray Wood and Marc Roper and Andrew Brooks},
 doi = {10.1016/s0950-5849(96)01139-1},
 journal = {Information and Software Technology},
 month = {apr},
 number = {4},
 pages = {285--295},
 publisher = {Elsevier {BV}},
 title = {Statistical power and its subcomponents {\textemdash} missing and misunderstood concepts in empirical software engineering research},
 url = {https://doi.org/10.1016%2Fs0950-5849%2896%2901139-1},
 volume = {39},
 year = {1997}
}

@article{Miller_2000,
 abstract = {Deriving reliable empirical results from a single experiment is an unlikely event. Hence to progress multiple experiments must be undertaken per hypothesis and the subsequent results effectively combined to produce a single reliable conclusion. Since results are quantitative in nature, a quantitative conclusion would be the optimal solution. Other disciplines use meta-analytic techniques to achieve this result. The treatise of this paper is: can meta-analysis be successfully applied to current software engineering experiments? The question is investigated by examining a series of experiments, which themselves to investigate — which defect-detection technique is best? Applying meta-analysis techniques to the software engineering data is relatively straightforward, but unfortunately the results are highly unstable, as the meta-analysis shows that the results are highly disparate and do not lead to a single reliable conclusion. The reason for this deficiency is the excessive variation within various components of the experiments. The paper outlines various ideas from other disciplines for controlling this variation and describes a number of recommendations for controlling and reporting empirical work to advance the discipline towards a position, where meta-analysis can be profitably employed.},
 author = {James Miller},
 doi = {10.1016/s0164-1212(00)00024-8},
 journal = {Journal of Systems and Software},
 month = {sep},
 number = {1},
 pages = {29--39},
 publisher = {Elsevier {BV}},
 title = {Applying meta-analytical procedures to software engineering experiments},
 url = {https://doi.org/10.1016%2Fs0164-1212%2800%2900024-8},
 volume = {54},
 year = {2000}
}

@article{Perry_1994,
 abstract = {In their efforts to determine how technology affects the software development process, researchers often overlook organizational and social issues. The authors report on two experiments to discover how developers spend their time. They describe how noncoding activities can use up development time and how even a reluctance to use e-mail can influence the development process. The first experiment was to see how programmers thought they spent their time by having them fill out a modified time card reporting their activities, which we called a time diary. In the second experiment, we used direct observation to calibrate and validate the use of time diaries, which helped us evaluate how time was actually being used.< >},
 author = {D.E. Perry and N.A. Staudenmayer and L.G. Votta},
 doi = {10.1109/52.300082},
 journal = {{IEEE} Software},
 month = {jul},
 number = {4},
 pages = {36--45},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {People, organizations, and process improvement},
 url = {https://doi.org/10.1109%2F52.300082},
 volume = {11},
 year = {1994}
}

@article{Pfleeger_1995,
 abstract = {The paper presents key activities necessary for designing and analyzing an experiment in software engineering. After explaining how to choose an appropriate research technique to fit project goals, the paper shows how to state a hypothesis and determine how much control is needed over the variables involved. If control is not possible, then a formal experiment is not possible; a case study may be a better approach. Next, the six stages of an experiment (conception, design, preparation, execution, analysis and dissemination) are described, with design examined in detail. Design considerations such as replication, randomization and local control are discussed, and design techniques such as crossing and nesting are explained. Finally, data analysis is shown to be a function both of the experimental design and the distribution of the data. Throughout, examples are given to show how the techniques are interpreted and used in software engineering.},
 author = {Shari Lawrence Pfleeger},
 doi = {10.1007/bf02249052},
 journal = {Annals of Software Engineering},
 month = {dec},
 number = {1},
 pages = {219--253},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Experimental design and analysis in software engineering},
 url = {https://doi.org/10.1007%2Fbf02249052},
 volume = {1},
 year = {1995}
}

@article{Pfleeger_1999,
 abstract = {Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.},
 author = {S.L Pfleeger},
 doi = {10.1016/s0164-1212(99)00031-x},
 journal = {Journal of Systems and Software},
 month = {jul},
 number = {2-3},
 pages = {111--124},
 publisher = {Elsevier {BV}},
 title = {Understanding and improving technology transfer in software engineering},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900031-x},
 volume = {47},
 year = {1999}
}

@article{Zelkowitz_1998,
 abstract = {Experimentation helps determine the effectiveness of proposed
theories and methods. However, computer science has not developed a
concise taxonomy of methods for demonstrating the validity of new
techniques. Experimentation is a crucial part of attribute evaluation
and can help determine whether methods used in accordance with some
theory during product development will result in software being as
effective as necessary. By looking at multiple examples of technology
validation, the authors develop a taxonomy for software engineering
experimentation that describes twelve different experimental approaches},
 author = {M.V. Zelkowitz and D.R. Wallace},
 doi = {10.1109/2.675630},
 journal = {Computer},
 month = {may},
 number = {5},
 pages = {23--31},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Experimental models for validating technology},
 url = {https://doi.org/10.1109%2F2.675630},
 volume = {31},
 year = {1998}
}

@incollection{Anderson_2001,
 abstract = {Requirements Evolution represents one of the major problems in developing computer-based systems. Current practice in Requirement Engineering relies on process-oriented methodologies, which lack of product features. The resulting scenario then is a collection of general methodologies, which do not take into account product features that may enhance our ability in monitoring and controlling Requirements Evolution. This paper shows empirical investigations of two industrial case studies. The results point out evolutionary product features and identify an Empirical Framework to analysing Requirements Evolution. This work represents a shift from process to product-oriented management of Requirements Evolution.},
 author = {Stuart Anderson and Massimo Felici},
 booktitle = {Product Focused Software Process Improvement},
 doi = {10.1007/3-540-44813-6_6},
 pages = {27--41},
 publisher = {Springer Berlin Heidelberg},
 title = {Requirements Evolution From Process to Product Oriented Management},
 url = {https://doi.org/10.1007%2F3-540-44813-6_6},
 year = {2001}
}

@article{Bozeman_2001,
 abstract = {Absent a set of indicators based on a set of valid constructs, it will remain difficult to determine the importance of strategic research partnerships (SRP) for the U.S. economy and the enterprise of science and technology in general. The purpose of this paper is to identify characteristics of a desirable policy indicator for strategic research partnerships. In choosing among the many possible conceptualizations and indicators of SRP, a framing question should be: "what is the public stake?" The fact that SRP activity has great economic import (still an open question) does not imply an equivalent government or public policy importance. This seems to imply, then, that SRP indicators should focus on such factors as the composition of R&D (e.g. do they shift the balance in available public domain research?), impacts of labor and human resources, tax implications (e.g. use of tax credits, foregone revenue and its impacts), and, especially, public-private partnerships (e.g. how are government-sponsored or -brokered SRPs different from others?). Copyright 2001 by Kluwer Academic Publishers},
 author = {Barry Bozeman and James S. Dietz},
 doi = {10.1023/a:1011143105426},
 journal = {The Journal of Technology Transfer},
 number = {4},
 pages = {385--393},
 publisher = {Springer Science and Business Media {LLC}},
 url = {https://doi.org/10.1023%2Fa%3A1011143105426},
 volume = {26},
 year = {2001}
}

@article{Diaz_1997,
 abstract = {Many organizations are using or considering the Capability
Maturity Model as a vehicle for software process improvement. But does
the CMM provide real benefits? The authors offer metrics and data that
show the results of Motorola's CMM usage},
 author = {M. Diaz and J. Sligo},
 doi = {10.1109/52.605934},
 journal = {{IEEE} Software},
 number = {5},
 pages = {75--81},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {How software process improvement helped Motorola},
 url = {https://doi.org/10.1109%2F52.605934},
 volume = {14},
 year = {1997}
}

@inproceedings{Fowler,
 abstract = {We built a prototype “transition package” to determine
if adoption of requirements management practices would be expedited by a
suite of RM-specific materials for change agents. The transition package
was a password-protected web site that included about 100 documents for
use in both performing and introducing requirements management
practices. These documents included examples, templates, checklists, and
guidance materials. Three frameworks-document type, Software Capability
Maturity Model Common Feature, and technology transition process
model-helped users make use of the documents. This paper describes the
prototype, summarizes the data gathered from an evaluation by reviewers
and piloting organizations, and comments on what the data reflect about
the need for and use of technology to support requirements management.
Finally, the potential of this approach for improving software
technology adoption and process improvement within organizations is
assessed based on these experiences},
 author = {P. Fowler and M. Patrick and A. Carleton and B. Merrin},
 booktitle = {Proceedings of {IEEE} International Symposium on Requirements Engineering: {RE} {\textquotesingle}98},
 doi = {10.1109/icre.1998.667819},
 publisher = {{IEEE} Comput. Soc},
 title = {Transition packages: an experiment in expediting the introduction of requirements management},
 url = {https://doi.org/10.1109%2Ficre.1998.667819}
}

@inproceedings{Greenspan,
 abstract = {Not Available},
 author = {S.J. Greenspan},
 booktitle = {Proceedings Fourth International Conference on Requirements Engineering. {ICRE} 2000. (Cat. No.98TB100219)},
 doi = {10.1109/icre.2000.855589},
 publisher = {{IEEE} Comput. Soc},
 title = {Why is it so easy to introduce requirements engineering technology transfer panels into mainstream practice?},
 url = {https://doi.org/10.1109%2Ficre.2000.855589}
}

@inproceedings{Kaindl,
 abstract = {The goal of this panel is to really understand the issue of why it is so difficult to introduce research results from requirements engineering into mainstream practice. The result should be a (research) agenda that helps us to bridge the gap between theory and practice and, finally, to reach the people in the trenches so that they improve the way they deal with requirements.},
 author = {H. Kaindl},
 booktitle = {Proceedings Fourth International Conference on Requirements Engineering. {ICRE} 2000. (Cat. No.98TB100219)},
 doi = {10.1109/icre.2000.855588},
 publisher = {{IEEE} Comput. Soc},
 title = {Why is it so difficult to introduce requirements engineering research results into mainstream requirements engineering practice?},
 url = {https://doi.org/10.1109%2Ficre.2000.855588}
}

@inproceedings{Mead,
 abstract = {Not Available},
 author = {N.R. Mead},
 booktitle = {Proceedings Fourth International Conference on Requirements Engineering. {ICRE} 2000. (Cat. No.98TB100219)},
 doi = {10.1109/icre.2000.855592},
 publisher = {{IEEE} Comput. Soc},
 title = {Why is it so difficult to introduce requirements engineering research results into mainstream requirements engineering practice?},
 url = {https://doi.org/10.1109%2Ficre.2000.855592}
}

@article{Morris_1998,
 abstract = {In September 1996 the Joint Research Centre of the European Commission held a workshop in Brussels on problems with industrial uptake from research and development projects in Requirements Engineering. Although there have, in this domain, been a number of research projects industrial uptake has rarely lived up to expectations. The workshop set out to investigate possible explanations for this and what potential mechanisms there may be for promoting industrial uptake of current and future Requirement Engineering projects. This paper has two functions: to describe the results of this workshops and to provide a framework to support planning for future RE activities and initiatives.},
 author = {Philip Morris and Marcelo Masera and Marc Wilikens},
 doi = {10.1007/bf02919966},
 journal = {Requirements Engineering},
 month = {jun},
 number = {2},
 pages = {79--83},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Requirements engineering and industrial uptake},
 url = {https://doi.org/10.1007%2Fbf02919966},
 volume = {3},
 year = {1998}
}

@article{Pfleeger_1999,
 abstract = {Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.},
 author = {S.L Pfleeger},
 doi = {10.1016/s0164-1212(99)00031-x},
 journal = {Journal of Systems and Software},
 month = {jul},
 number = {2-3},
 pages = {111--124},
 publisher = {Elsevier {BV}},
 title = {Understanding and improving technology transfer in software engineering},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900031-x},
 volume = {47},
 year = {1999}
}

@inproceedings{Sampaio_do_Prado_Leite_2000,
 abstract = {Not Available},
 author = {J.C. Sampaio do Prado Leite},
 booktitle = {Proceedings Fourth International Conference on Requirements Engineering. {ICRE} 2000. (Cat. No.98TB100219)},
 doi = {10.1109/icre.2000.855591},
 publisher = {{IEEE}},
 title = {Is there a gap between {RE} research and {RE} practice ?},
 url = {https://doi.org/10.1109%2Ficre.2000.855591},
 year = {2000}
}

@inproceedings{Wohlwend,
 abstract = {The Schlumberger Laboratory for Computer Science (SLCS) was formed in 1989 as a corporate-wide resource to enhance the quality and creativity of software products within Schlumberger and to improve the productivity of software development. Because of a wide range of needs, it was necessary to decide which techniques SLCS's small software improvement team could use most effectively. The authors describe the choices made and the effectiveness of those choices in work with Schlumberger's engineering organizations. Through the motivation efforts of a small group, productive changes have occurred across the company. Improvements were made in many development areas, including project planning and requirements management. The catalysts behind these advances included capability assessments, training, and collaboration},
 author = {H. Wohlwend and S. Rosenbaum},
 booktitle = {Proceedings of 1993 15th International Conference on Software Engineering},
 doi = {10.1109/icse.1993.346042},
 publisher = {{IEEE} Comput. Soc. Press},
 title = {Software improvements in an international company},
 url = {https://doi.org/10.1109%2Ficse.1993.346042}
}

@article{2000,
 abstract = {},
 doi = {10.1016/s0898-1221(00)90203-7},
 journal = {Computers {\&}amp$\mathsemicolon$ Mathematics with Applications},
 month = {jul},
 number = {2-3},
 pages = {418},
 publisher = {Elsevier {BV}},
 title = {Experimentation in software engineering: An introduction},
 url = {https://doi.org/10.1016%2Fs0898-1221%2800%2990203-7},
 volume = {40},
 year = {2000}
}

@article{Bril_2005,
 abstract = {Software architecture plays a vital role in the development (and hence maintenance) of large complex systems (containing millions of lines of code) with a long lifetime. It is therefore required that the software architecture is also maintained, i.e., sufficiently documented, clearly communicated, and explicitly controlled during its life-cycle. In our experience, these requirements cannot be met without appropriate support.
Commercial-off-the-shelf support for architectural maintenance is still scarcely available, if at all, implying the need to develop appropriate proprietary means. In this paper, we reflect upon software architecture maintenance taken within three organizations within Philips that develop professional systems. We extensively describe the experience gained with introducing and embedding of architectural support in these three organizations. We focus on architectural support in the area of software architecture recovery, visualization, analysis, and verification.
In our experience, the support must be carried by a number of pillars of software development, and all of these pillars have to go through a change process to ensure sustainable embedding. Managing these changes requires several key roles to be fulfilled in the organization: a champion, a company angel, a change agent, and a target. We call our reflection model C-POSH, which is an acronym for Change management of the four identified pillars of software development: Process, Organization, Software development environment, and Humans. Our experiences will be presented in terms of the C-POSH model. Copyright},
 author = {R. J. Bril and R. L. Krikhaar and A. Postma},
 doi = {10.1002/smr.304},
 journal = {Journal of Software Maintenance and Evolution: Research and Practice},
 month = {jan},
 number = {1},
 pages = {3--25},
 publisher = {Wiley},
 title = {Architectural support in industry: a reflection using C-{POSH}},
 url = {https://doi.org/10.1002%2Fsmr.304},
 volume = {17},
 year = {2005}
}

@article{Hall_1997,
 abstract = {Increasingly organisations are foregoing an ad hoc approach to
metrics in favor of complete metrics programs. The authors identify
consensus requirements for metric program success and examine how
programs in two organisations measured up},
 author = {T. Hall and N. Fenton},
 doi = {10.1109/52.582975},
 journal = {{IEEE} Software},
 number = {2},
 pages = {55--65},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Implementing effective software metrics programs},
 url = {https://doi.org/10.1109%2F52.582975},
 volume = {14},
 year = {1997}
}

@inproceedings{Kitchenham,
 abstract = {Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.},
 author = {B.A. Kitchenham and T. Dyba and M. Jorgensen},
 booktitle = {Proceedings. 26th International Conference on Software Engineering},
 doi = {10.1109/icse.2004.1317449},
 publisher = {{IEEE} Comput. Soc},
 title = {Evidence-based software engineering},
 url = {https://doi.org/10.1109%2Ficse.2004.1317449}
}

@article{Pfleeger_1999,
 abstract = {Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.},
 author = {S.L Pfleeger},
 doi = {10.1016/s0164-1212(99)00031-x},
 journal = {Journal of Systems and Software},
 month = {jul},
 number = {2-3},
 pages = {111--124},
 publisher = {Elsevier {BV}},
 title = {Understanding and improving technology transfer in software engineering},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900031-x},
 volume = {47},
 year = {1999}
}

@article{Pfleeger_2000,
 abstract = {We can learn much from the business community about effective
technology transfer. In particular, understanding the interests of
different types of adopters can suggest to us the different kinds of
evidence needed to convince someone to try an innovative technology. At
the same time, the legal community offers us advice about what kinds of
evidence are needed to build convincing cases that an innovation is an
improvement over current practice. The article examines why and how we
make technology selection decisions and also considers how evidence
supporting these decisions helps or hinders the adoption of new
technology},
 author = {S.L. Pfleeger and W. Menezes},
 doi = {10.1109/52.819965},
 journal = {{IEEE} Software},
 number = {1},
 pages = {27--33},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Marketing technology to software practitioners},
 url = {https://doi.org/10.1109%2F52.819965},
 volume = {17},
 year = {2000}
}

@article{Porter_1995,
 abstract = {Accepting a fixed trade-off between environmental regulation and competitiveness unnecessarily raises costs and slows down environmental progress. Studies finding high environmental compliance costs have traditionally focused on static cost impacts, ignoring any offsetting productivity benefits from innovation. They typically overestimated compliance costs, neglected innovation offsets, and disregarded the affected industry's initial competitiveness. Rather than simply adding to cost, properly crafted environmental standards can trigger innovation offsets, allowing companies to improve their resource productivity. Shifting the debate from pollution control to pollution prevention was a step forward. It is now necessary to make the next step and focus on resource productivity. Copyright 1995 by American Economic Association.},
 author = {Michael E Porter and Claas van der Linde},
 doi = {10.1257/jep.9.4.97},
 journal = {Journal of Economic Perspectives},
 month = {nov},
 number = {4},
 pages = {97--118},
 publisher = {American Economic Association},
 title = {Toward a New Conception of the Environment-Competitiveness Relationship},
 url = {https://doi.org/10.1257%2Fjep.9.4.97},
 volume = {9},
 year = {1995}
}

@misc{Rombach_2002,
 abstract = {The Fraunhofer-Gesellschaft is the leading organization for applied research and technology transfer in Germany and Europe. Approximately 9,000 people are employed at 48 institutes throughout Germany, most of them scientists and engineers. International collaboration is promoted through Fraunhofer branches in the United States and Asia. The association's headquarters are in Munich, Germany. Fraunhofer institutes develop products and processes in eight focal research fields ranging from material sciences and production technology to information and communication technology and life sciences. Individual solutions are generated in close collaboration with industrial partners.

Keywords:

software development;
predictable quality;
product lines;
optimization;
database management;
learning organizations;
education;
training;
systems security;
organizational set-up},
 author = {H. Dieter Rombach},
 doi = {10.1002/0471028959.sof135},
 month = {jan},
 publisher = {John Wiley {\&} Sons, Inc.},
 title = {Fraunhofer Institute for Experimental Software Engineering ({IESE})},
 url = {https://doi.org/10.1002%2F0471028959.sof135},
 year = {2002}
}

@book{Sametinger_1997,
 abstract = {},
 author = {Johannes Sametinger},
 doi = {10.1007/978-3-662-03345-6},
 publisher = {Springer Berlin Heidelberg},
 title = {Software Engineering with Reusable Components},
 url = {https://doi.org/10.1007%2F978-3-662-03345-6},
 year = {1997}
}

@article{Senge_1991,
 abstract = {Incl. bibl., index.},
 author = {Peter M. Senge},
 doi = {10.1002/pfi.4170300510},
 journal = {Performance $\mathplus$ Instruction},
 month = {may},
 number = {5},
 pages = {37--37},
 publisher = {Wiley},
 title = {The fifth discipline, the art and practice of the learning organization},
 url = {https://doi.org/10.1002%2Fpfi.4170300510},
 volume = {30},
 year = {1991}
}

@article{Stubrich_1993,
 abstract = {},
 author = {Gustavo Stubrich},
 doi = {10.1016/0022-5428(93)90045-q},
 journal = {The Columbia Journal of World Business},
 month = {jun},
 number = {2},
 pages = {108--109},
 publisher = {Elsevier {BV}},
 title = {},
 url = {https://doi.org/10.1016%2F0022-5428%2893%2990045-q},
 volume = {28},
 year = {1993}
}

@article{Volberda_2003,
 abstract = {The extensive selection-adaptation literature spans diverse theoretical perspectives, but is inconclusive on the role of managerial intentionality in organizational adaptation. Indeed this voluminous literature has more to say about selection and sources and causes of structural inertia than about self-renewing organizations that might counteract such inertia. In this introductory essay, we identify four co-evolutionary generative mechanisms (engines) - naïve selection, managed selection, hierarchical renewal and holistic renewal - which illustrate the extensive range of evolutionary paths that can take place in a population of organizations. In particular, the managed selection engine provides the foundations of the underlying principles of co-evolving self-renewing organizations: managing internal rates of change, optimizing self-organization, and balancing concurrent exploration and exploitation. However, it is altogether clear that empirical co-evolution research represents the next frontier for empirically resolving the adaptation selection debate. The essay concludes with a discussion of requirements for co-evolutionary empirical research and introduces the empirical papers in this Special Research Symposium. Copyright 2003 Blackwell Publishing Ltd..},
 author = {Henk W. Volberda and Arie Y. Lewin},
 doi = {10.1046/j.1467-6486.2003.00414.x},
 journal = {Journal of Management Studies},
 month = {dec},
 number = {8},
 pages = {2111--2136},
 publisher = {Wiley},
 title = {Co-evolutionary Dynamics Within and Between Firms: From Evolution to Co-evolution},
 url = {https://doi.org/10.1046%2Fj.1467-6486.2003.00414.x},
 volume = {40},
 year = {2003}
}

@article{2005,
 abstract = {},
 doi = {10.1038/4351138b},
 journal = {Nature},
 month = {jun},
 number = {7046},
 pages = {1138--1138},
 publisher = {Springer Science and Business Media {LLC}},
 title = {Crystal clear},
 url = {https://doi.org/10.1038%2F4351138b},
 volume = {435},
 year = {2005}
}

@inproceedings{Armarego_2005,
 abstract = {Both the increasing knowledge needed to practise as a professional, and the accelerating rate of change within the discipline suggest that traditional learning models may not address the requirements of learners. Problem-based learning (PBL) and design studios (DS) are two approaches that focus on learners developing characteristics of lifelong learning. This tutorial explores a Problem-based Design Studio (PbDS) model of learning. The goal is to enable participants to gain some understanding of the model so as to evaluate its' applicability in their teaching/learning context.},
 author = {Jocelyn Armarego and Sally Clarke},
 booktitle = {18th Conference on Software Engineering Education {\&}amp$\mathsemicolon$ Training ({CSEET}{\textquotesingle}05)},
 doi = {10.1109/cseet.2005.24},
 month = {apr},
 publisher = {{IEEE}},
 title = {Problem-based Design Studios for Undergraduate {SE} Education},
 url = {https://doi.org/10.1109%2Fcseet.2005.24},
 year = {2005}
}

@inproceedings{B_rstler_2005,
 abstract = {CRC-cards are a lightweight approach to collaborative object-oriented modeling. They have been adopted by many educators and trainers to teach early object-oriented design. Reports in the literature are generally positive. So is our own experience. However, over the years, we have noticed many subtle problems and issues that have largely gone unnoticed in the literature.In this paper, we discuss the problems and issues we experienced when teaching CRC-cards to novices. Two major sources of problems can be traced back to the CRC-card role-play. One is the usage of CRC-cards as substitutes for actual objects during the scenario role-play and the other the difficulty to document or trace the scenario role-play ``on the fly". We propose a new type of diagram to support the role-play activities and to overcome these problems. Our experience so far is quite positive. Novices have fewer problems with role-play activities when using these diagrams. Teaching and learning the new type of diagram adds only little overhead to the overall CRC-approach.We also provide general guidelines for CRC-card usage. Although our improvements are aimed at novices, we believe that the proposed diagram is useful even for professional software development.},
 author = {Jürgen Börstler},
 booktitle = {Companion to the 20th annual {ACM} {SIGPLAN} conference on Object-oriented programming, systems, languages, and applications},
 doi = {10.1145/1094855.1094973},
 month = {oct},
 publisher = {{ACM}},
 title = {Improving {CRC}-card role-play with role-play diagrams},
 url = {https://doi.org/10.1145%2F1094855.1094973},
 year = {2005}
}

@article{Baker_2005,
 abstract = {The typical software engineering course consists of lectures in which concepts and theories are conveyed, along with a small “toy” software engineering project which attempts to give students the opportunity to put this knowledge into practice. Although both of these components are essential, neither one provides students with adequate practical knowledge regarding the process of software engineering. Namely, lectures allow only passive learning and projects are so constrained by the time and scope requirements of the academic environment that they cannot be large enough to exhibit many of the phenomena occurring in real-world software engineering processes. To address this problem, we have developed Problems and Programmers, an educational card game that simulates the software engineering process and is designed to teach those process issues that are not sufficiently highlighted by lectures and projects. We describe how the game is designed, the mechanics of its game play, and the results of an experiment we conducted involving students playing the game.},
 author = {Alex Baker and Emily Oh Navarro and Andr{\'{e}} van der Hoek},
 doi = {10.1016/j.jss.2004.02.033},
 journal = {Journal of Systems and Software},
 month = {feb},
 number = {1-2},
 pages = {3--16},
 publisher = {Elsevier {BV}},
 title = {An experimental card game for teaching software engineering processes},
 url = {https://doi.org/10.1016%2Fj.jss.2004.02.033},
 volume = {75},
 year = {2005}
}

@inproceedings{Dean_1995,
 abstract = {By acting as a client, the instructor in a formal methods course can interact with students to develop general attitudes and skills, particularly modeling skills. The students learn to appreciate that formal methods comprise a range of tools and ideas which can greatly enhance software development at all stages, especially in requirements capture. A realistic case study is used which does not depend on an over-simplified file processing system.},
 author = {Neville Dean and Michael G. Hinchey},
 booktitle = {Proceedings of the twenty-sixth {SIGCSE} technical symposium on Computer science education},
 doi = {10.1145/199688.199831},
 month = {mar},
 publisher = {{ACM}},
 title = {Introducing formal methods through role-playing},
 url = {https://doi.org/10.1145%2F199688.199831},
 year = {1995}
}

@inproceedings{Drappa_2000,
 abstract = {},
 author = {Anke Drappa and Jochen Ludewig},
 booktitle = {Proceedings of the 22nd international conference on Software engineering  - {ICSE} {\textquotesingle}00},
 doi = {10.1145/337180.337203},
 publisher = {{ACM} Press},
 title = {Simulation in software engineering training},
 url = {https://doi.org/10.1145%2F337180.337203},
 year = {2000}
}

@article{Hallam_1955,
 abstract = {},
 author = {Beverly Hallam and Helen Cabot Miles and Edgar Dale},
 doi = {10.2307/3184288},
 journal = {Art Education},
 month = {feb},
 number = {2},
 pages = {15},
 publisher = {Informa {UK} Limited},
 title = {Audio Visual Methods in Teaching},
 url = {https://doi.org/10.2307%2F3184288},
 volume = {8},
 year = {1955}
}

@inproceedings{Jun_Suk_Oh_2005,
 abstract = {In this paper, the target of code review is project management system (PMS), developed by a studio project in a software engineering master's program, and the focus is on finding defects not only in view of development standards, i.e., design rule and naming rule, but also in view of quality attributes of PMS, i.e., performance and security. From the review results, a few lessons are learned. First, defects which had not been found in the test stage of PMS development could be detected in this code review. These are hidden defects that affect system quality and that are difficult to find in the test. If the defects found in this code review had been fixed before the test stage of PMS development, productivity and quality enhancement of the project would have been improved. Second, manual review takes much longer than an automated one. In this code review, general check items were checked by automation tool, while project-specific ones were checked by manual method. If project-specific check items could also be checked by automation tool, code review and verification work after fixing the defects would be conducted very efficiently. Reflecting on this idea, an evolution model of code review is studied, which eventually seeks fully automated review as an optimized code review.},
 author = {Jun-Suk Oh and  Ho-Jin Choi},
 booktitle = {Fourth Annual {ACIS} International Conference on Computer and Information Science ({ICIS}{\textquotesingle}05)},
 doi = {10.1109/icis.2005.17},
 publisher = {{IEEE}},
 title = {A reflective practice of automated and manual code reviews for a studio project},
 url = {https://doi.org/10.1109%2Ficis.2005.17},
 year = {2005}
}

@article{Kazman_1996,
 abstract = {Despite advances in clarifying high level design needs, analyzing
a system's ability to meet desired quality criteria is still difficult.
The authors propose using scenarios to make analysis more
straightforward. In their case study report, they analyze lessons
learned with this approach. They developed the Software Architecture
Analysis Method, an approach that uses scenarios to gain information
about a system's ability to meet desired quality attributes. Scenarios
are brief narratives of expected or anticipated system uses from both
user and developer views and they provide a look at how the system
satisfies quality attributes in various use contexts},
 author = {R. Kazman and G. Abowd and L. Bass and P. Clements},
 doi = {10.1109/52.542294},
 journal = {{IEEE} Software},
 number = {6},
 pages = {47--55},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {Scenario-based analysis of software architecture},
 url = {https://doi.org/10.1109%2F52.542294},
 volume = {13},
 year = {1996}
}

@techreport{Kazman_2003,
 abstract = {},
 author = {Rick Kazman and Robert L. Nord and Mark Klein},
 doi = {10.21236/ada421679},
 month = {sep},
 publisher = {Defense Technical Information Center},
 title = {A Life-Cycle View of Architecture Analysis and Design Methods},
 url = {https://doi.org/10.21236%2Fada421679},
 year = {2003}
}

@article{Kruchten_1995,
 abstract = {The 4 + 1 View Model describes software architecture using five concurrent views, each of which addresses a specific set of concerns: The logical view describes the design's object model, the process view describes the design's concurrency and synchronization aspects; the physical view describes the mapping of the software onto the hardware and shows the system's distributed aspects, and the development view describes the software's static organization in the development environment. Software designers can organize the description of their architectural decisions around these four views and then illustrate them with a few selected use cases, or scenarios, which constitute a fifth view. The architecture is partially evolved from these scenarios.The 4+1 View Model allows various stakeholders to find what they need in the software architecture. System engineers can approach it first from the physical view, then the process view; end users, customers, and data specialists can approach it from the logical view; and project managers and software-configuration staff members can approach it from the development view.},
 author = {P.B. Kruchten},
 doi = {10.1109/52.469759},
 journal = {{IEEE} Software},
 number = {6},
 pages = {42--50},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {The 4$\mathplus$1 View Model of architecture},
 url = {https://doi.org/10.1109%2F52.469759},
 volume = {12},
 year = {1995}
}

@article{Kuhn_1998,
 abstract = {Some software designers have recently turned for inspiration to
the process of building design to improve development practices and
increase software's usefulness and effectiveness. Architects' education
revolves around the studio course, which promotes: project based work on
complex and open ended problems; very rapid iteration of design
solutions; frequent formal and informal critique; consideration of
heterogeneous issues; the use of precedent and thinking about the whole;
the creative use of constraints; and the central importance of design
media. M. Kapor (1991) suggested that software practitioners needed to
“rethink the fundamentals of how software is made” and
proposed the architect's role in building design as a fruitful analogy
for software professionals seeking to reform software practice. This
analogy helps us focus on usefulness, usability, user needs and
practices, and other technical and nontechnical aspects of good software
design. It highlights concerns about people's lives and work practices
and how people “inhabit” systems. Several authors have
explored similarities and differences between software design and
building design, including some who have pursued the software
implications of architect Christopher Alexander's design patterns},
 author = {S. Kuhn},
 doi = {10.1109/52.663788},
 journal = {{IEEE} Software},
 number = {2},
 pages = {65--71},
 publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
 title = {The software design studio: an exploration},
 url = {https://doi.org/10.1109%2F52.663788},
 volume = {15},
 year = {1998}
}

@inproceedings{Lassing,
 abstract = {Software architecture analysis helps us assess the quality of a
software system at an early stage. We describe a case study of software
architecture analysis that we have performed to assess the flexibility
of a large administrative system. Our analysis was based on scenarios,
representing possible changes to the requirements of the system and its
environment. Assessing the effect of these scenarios provides insight
into the flexibility of the system. One of the problems is to express
the effect of a scenario in such a way that it provides insight into the
complexity of the necessary changes. Part of our research is directed at
developing an instrument for doing just that. This instrument is applied
in the analysis presented},
 author = {N. Lassing and D. Rijsenbrij and H. van Vliet},
 booktitle = {Proceedings Sixth Asia Pacific Software Engineering Conference ({ASPEC}{\textquotesingle}99) (Cat. No.{PR}00509)},
 doi = {10.1109/apsec.1999.809608},
 publisher = {{IEEE} Comput. Soc},
 title = {Towards a broader view on software architecture analysis of flexibility},
 url = {https://doi.org/10.1109%2Fapsec.1999.809608}
}

@incollection{Nawrocki_2006,
 abstract = {Extreme89 is a simulation game designed to introduce software teams – programmers and customers – to Extreme Programming practices.
The game is run by a moderator and lasts 89 minutes – this is the reason why we named it Extreme89. Several teams build-up of customer representative and programmers compete to earn maximum number of points. Teams earn points
for delivering properly produced artifacts. Artifacts in the game correspond to software modules delivered to customer in
real software projects. Every artifact in the game is assigned a Fibonacci-like function. Manual computing values of the functions
performed by the programmers substitutes real programming. Rules of Extreme89 closely correspond to XP practices. The game has two releases while each release is build-up of two increments. Extreme89 with its atmosphere of the competition and time-compressed active lesson of XP was successfully introduced to Computer Science
students at Poznan University of Technology.},
 author = {Jerzy Nawrocki and Adam Wojciechowski},
 booktitle = {Rapid Integration of Software Engineering Techniques},
 doi = {10.1007/11751113_20},
 pages = {278--287},
 publisher = {Springer Berlin Heidelberg},
 title = {Extreme89: An {XP} War Game},
 url = {https://doi.org/10.1007%2F11751113_20},
 year = {2006}
}

@article{Pfleeger_1999,
 abstract = {Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.},
 author = {S.L Pfleeger},
 doi = {10.1016/s0164-1212(99)00031-x},
 journal = {Journal of Systems and Software},
 month = {jul},
 number = {2-3},
 pages = {111--124},
 publisher = {Elsevier {BV}},
 title = {Understanding and improving technology transfer in software engineering},
 url = {https://doi.org/10.1016%2Fs0164-1212%2899%2900031-x},
 volume = {47},
 year = {1999}
}

@inproceedings{Ramakrishan_2003,
 abstract = {This paper presents our rationale in setting up an innovative studio lab called the MUSE (Monash University Software Engineering) Studio Lab for our final year undergraduate Software Engineering students in 2002, an evaluation of the outcomes for 2002 and plans for 2003 and beyond. We describe the Monash University Software Engineering (MUSE) Strategy to make the MUSE culture and MUSE agile process branding visible through the MUSE Studio Lab for the learners, academics, management and the outside world of academia and industry. This strategy is aimed at supporting a continuum of learning styles in three units in the final year of our Bachelor of Software Engineering Program. The teaching/learning model used is a predominantly constructivist/student-centered/active learning approach for the Capstone SE project (Studio unit), and a constructivist approach for the Honours Research thesis unit. A more traditional teaching approach is used for the Systems Validation, Verification, Quality & Standards unit based on traditional instructional design model which is objectivist in view of learning. We also provide details of the the middle weight process that student groups follow in their full year SE Capstone project. At MUSE Studio, we have fused pedagogy with technology.},
 author = {Sita Ramakrishan},
 booktitle = {Proceedings of the 8th annual conference on Innovation and technology in computer science education},
 doi = {10.1145/961511.961521},
 month = {jun},
 publisher = {{ACM}},
 title = {{MUSE} studio lab and innovative software engineering capstone project experience},
 url = {https://doi.org/10.1145%2F961511.961521},
 year = {2003}
}

@inproceedings{Richardson_2006,
 abstract = {The Global Studio Project integrated the work of Software Engineering students spread across four countries into a single project and represented, for most of the students, their first major "real-world" development experience. Interviews indicated that the major areas of learning were informal skills that included learning to establish and work effectively within a team, learning how to react quickly to frequent changes in requirements, architecture and organization, and learning to manage and optimize communications. Since all these skills require rapid reaction to unpredictable factors, we view them as improvisation and discuss the role of experiential education in facilitating improvisation.},
 author = {Ita Richardson and Allen E. Milewski and Neel Mullick and Patrick Keil},
 booktitle = {Proceedings of the 28th international conference on Software engineering},
 doi = {10.1145/1134285.1134390},
 month = {may},
 publisher = {{ACM}},
 title = {Distributed development},
 url = {https://doi.org/10.1145%2F1134285.1134390},
 year = {2006}
}

@incollection{Tesoriero_Tvedt_2004,
 abstract = {As software systems become increasingly complex, the need to investigate and evaluate them at high levels of abstraction becomes more important. When systems are very complex, evaluating the system from an architectural level is necessary in order to understand the structure and interrelationships among the components of the system. There are several existing approaches available for software architecture evaluation. Some of these techniques, pre-implementation software architectural evaluations, are performed before the system is implemented. Others, implementation-oriented software architectural evaluations, are performed after a version of the system has been implemented. This chapter briefly describes the concepts of software architecture and software architectural evaluations, describes a new process for software architectural evaluation, provides results from two case studies where this process was applied, and presents areas for future work.},
 author = {Roseanne Tesoriero Tvedt and Patricia Costa and Mikael Lindvall},
 booktitle = {Advances in Computers},
 doi = {10.1016/s0065-2458(03)61001-6},
 pages = {1--43},
 publisher = {Elsevier},
 title = {Evaluating Software Architectures},
 url = {https://doi.org/10.1016%2Fs0065-2458%2803%2961001-6},
 year = {2004}
}

